{"cells":[{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":33304,"status":"ok","timestamp":1657682801470,"user":{"displayName":"darwin zhao","userId":"12087337285275609974"},"user_tz":-480},"id":"9G-sKmhe7MgT","outputId":"c8e64738-7fdd-416f-842a-1c4f1b232816"},"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Hit:6 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n","Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Hit:8 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n","Hit:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Fetched 88.7 kB in 2s (44.6 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","cuda is already the newest version (11.7.0-1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 69 not upgraded.\n","\u001b[33mWARNING: Skipping cupy-cuda102 as it is not installed.\u001b[0m\n","Found existing installation: albumentations 1.2.1\n","Uninstalling albumentations-1.2.1:\n","  Successfully uninstalled albumentations-1.2.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: cupy-cuda117 in /usr/local/lib/python3.7/dist-packages (10.6.0)\n","Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.7/dist-packages (from cupy-cuda117) (0.8)\n","Requirement already satisfied: numpy<1.25,>=1.18 in /usr/local/lib/python3.7/dist-packages (from cupy-cuda117) (1.21.6)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: segmentation_models_pytorch in /usr/local/lib/python3.7/dist-packages (0.2.1)\n","Requirement already satisfied: efficientnet-pytorch==0.6.3 in /usr/local/lib/python3.7/dist-packages (from segmentation_models_pytorch) (0.6.3)\n","Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from segmentation_models_pytorch) (0.12.0+cu113)\n","Requirement already satisfied: timm==0.4.12 in /usr/local/lib/python3.7/dist-packages (from segmentation_models_pytorch) (0.4.12)\n","Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.7/dist-packages (from segmentation_models_pytorch) (0.7.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (1.11.0+cu113)\n","Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (2.5.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.64.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (4.1.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.0.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement cv2 (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for cv2\u001b[0m\n","\u001b[31mERROR: Invalid requirement: 'opencv-python-headless=4.5.5.64'\n","Hint: = is not a valid operator. Did you mean == ?\u001b[0m\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: opencv-python-headless==4.5.2.52 in /usr/local/lib/python3.7/dist-packages (4.5.2.52)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless==4.5.2.52) (1.21.6)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: opencv-python==4.1.2.30 in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==4.1.2.30) (1.21.6)\n","start1\n","/usr/lib64-nvidia\n","start2\n","/usr/lib/x86_64-linux-gnu/libcuda.so.515.48.07\n","/usr/lib/x86_64-linux-gnu/libcuda.so.1\n","/usr/local/cuda-11.0/doc/man/man7/libcuda.so.7\n","/usr/local/cuda-10.1/doc/man/man7/libcuda.so.7\n","/usr/local/cuda-10.0/doc/man/man7/libcuda.so.7\n","/usr/local/cuda-11.1/compat/libcuda.so.1\n","/usr/local/cuda-11.1/compat/libcuda.so.455.45.01\n","/usr/lib64-nvidia/libcuda.so.460.32.03\n","/usr/lib64-nvidia/libcuda.so.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/albu/albumentations\n","  Cloning https://github.com/albu/albumentations to /tmp/pip-req-build-kktkad5u\n","  Running command git clone -q https://github.com/albu/albumentations /tmp/pip-req-build-kktkad5u\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.1) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.1) (1.4.1)\n","Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.1) (0.18.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.1) (3.13)\n","Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.1) (0.0.4)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.1) (4.1.2.30)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations==1.2.1) (1.0.2)\n","Requirement already satisfied: opencv-python-headless>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations==1.2.1) (4.5.2.52)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations==1.2.1) (4.1.1)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (2.4.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (2.6.3)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (3.2.2)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (1.3.0)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (7.1.2)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (2021.11.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.2.1) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.2.1) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.2.1) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.2.1) (1.4.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.2.1) (1.15.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.2.1) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.2.1) (1.1.0)\n","Building wheels for collected packages: albumentations\n","  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for albumentations: filename=albumentations-1.2.1-py3-none-any.whl size=116663 sha256=ac5b2564f538c685bb180acde6aa95b1539b6a42e060949b26c6ec960dd08ee9\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-q9ad85pb/wheels/63/11/1a/c77caf3ae9b9b6d57b3ee5e6a41a50f3bc12c66a70f6b90bf0\n","Successfully built albumentations\n","Installing collected packages: albumentations\n","Successfully installed albumentations-1.2.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["albumentations"]}}},"metadata":{}}],"source":["!apt-get update\n","!apt-get install cuda\n","\n","!pip uninstall -y cupy-cuda102\n","!pip uninstall -y albumentations\n","# !pip uninstall -y cupy-cuda117\n","\n","\n","\n","# !pip install cupy-cuda102\n","#!nvcc --version\n","!pip install cupy-cuda117\n","\n","\n","!export CUDA_VISIBLE_DEVICES=0;\n","!pip install segmentation_models_pytorch\n","!pip install cv2\n","!pip uninstall opencv-python-headless=4.5.5.64\n","!pip install opencv-python-headless==4.5.2.52\n","!pip install opencv-python==4.1.2.30\n","# !pip install cupy\n","!echo \"start1\"\n","!echo $LD_LIBRARY_PATH #path\n","# !export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.0/lib64/\n","!echo \"start2\"\n","!sudo find /usr -name 'libcuda.so.*' #version\n","!pip install -U git+https://github.com/albu/albumentations --no-cache-dir"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1657682801471,"user":{"displayName":"darwin zhao","userId":"12087337285275609974"},"user_tz":-480},"id":"iiRhZEH_tHTR"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qjHmLO1E7NMC","outputId":"0d1eb171-9eee-4d35-9a26-11f796a99c1e"},"outputs":[{"output_type":"stream","name":"stdout","text":["################################################################################\n","###### Fold: 0\n","################################################################################\n"]},{"output_type":"stream","name":"stderr","text":["Train : 100%|██████████| 533/533 [06:24<00:00,  1.39it/s]"]},{"output_type":"stream","name":"stdout","text":["lr: 0.0000\n","loss: 16.485, focal_all: 16.485\n"]},{"output_type":"stream","name":"stderr","text":["\n","Valid :  82%|████████▏ | 272/331 [04:24<00:28,  2.10it/s]"]}],"source":["###############################################################\n","##### @Title:  UWMGI baseline\n","##### @Time:  2022/6/3\n","##### @Author: frank\n","##### @Describe: \n","        #  part0: data preprocess\n","        #  part1: build_transforme() & build_dataset() & build_dataloader()\n","        #  part2: build_model()\n","        #  part3: build_loss()\n","        #  part4: build_metric()\n","        #  part5: train_one_epoch() & valid_one_epoch() & test_one_epoch()\n","###############################################################\n","import os\n","import pdb\n","from tkinter.messagebox import NO\n","import cv2\n","import time\n","import glob\n","import random\n","\n","from cv2 import transform\n","import cupy as cp # https://cupy.dev/ => pip install cupy-cuda102\n","import numpy as np\n","import pandas as pd\n","from glob import glob\n","from tqdm import tqdm\n","\n","import torch # PyTorch\n","from torch.utils.data import Dataset, DataLoader\n","from torch.cuda import amp # https://pytorch.org/docs/stable/notes/amp_examples.html\n","\n","from sklearn.model_selection import StratifiedGroupKFold # Sklearn\n","import albumentations as A # Augmentations\n","import segmentation_models_pytorch as smp # smp\n","\n","from keras.callbacks import LearningRateScheduler\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","\n","\n","\n","def set_seed(seed=42):\n","    ##### why 42? The Answer to the Ultimate Question of Life, the Universe, and Everything is 42.\n","    random.seed(seed) # python\n","    np.random.seed(seed) # numpy\n","    torch.manual_seed(seed) # pytorch\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","###############################################################\n","##### part0: data preprocess\n","###############################################################\n","def get_metadata(row):\n","    data = row['id'].split('_')\n","    case = int(data[0].replace('case',''))\n","    day = int(data[1].replace('day',''))\n","    slice_ = int(data[-1])\n","    row['case'] = case\n","    row['day'] = day\n","    row['slice'] = slice_\n","    return row\n","\n","def path2info(row):\n","    path = row['image_path']\n","    data = path.split('/')\n","    slice_ = int(data[-1].split('_')[1])\n","    case = int(data[-3].split('_')[0].replace('case',''))\n","    day = int(data[-3].split('_')[1].replace('day',''))\n","    width = int(data[-1].split('_')[2])\n","    height = int(data[-1].split('_')[3])\n","    row['height'] = height\n","    row['width'] = width\n","    row['case'] = case\n","    row['day'] = day\n","    row['slice'] = slice_\n","    # row['id'] = f'case{case}_day{day}_slice_{slice_}'\n","    return row\n","\n","def mask2rle(msk, thr=0.5):\n","    '''\n","    img: numpy array, 1 - mask, 0 - background\n","    Returns run length as string formated\n","    '''\n","    msk    = cp.array(msk)\n","    pixels = msk.flatten()\n","    pad    = cp.array([0])\n","    pixels = cp.concatenate([pad, pixels, pad])\n","    runs   = cp.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)\n","\n","def masks2rles(msks, ids, heights, widths):\n","    pred_strings = []; pred_ids = []; pred_classes = [];\n","    for idx in range(msks.shape[0]):\n","        height = heights[idx].item()\n","        width = widths[idx].item()\n","        msk = cv2.resize(msks[idx], \n","                        dsize=(width, height), \n","                        interpolation=cv2.INTER_NEAREST) # back to original shape\n","        rle = [None]*3\n","        for midx in [0, 1, 2]:\n","            rle[midx] = mask2rle(msk[...,midx])\n","        pred_strings.extend(rle)\n","        pred_ids.extend([ids[idx]]*len(rle))\n","        pred_classes.extend(['large_bowel', 'small_bowel', 'stomach'])\n","    return pred_strings, pred_ids, pred_classes\n","\n","###############################################################\n","##### part1: build_transforms & build_dataset & build_dataloader\n","###############################################################\n","def build_transforms(CFG):\n","    data_transforms = {\n","        \"train\": A.Compose([\n","            A.OneOf([\n","                A.Resize(*CFG.img_size, interpolation=cv2.INTER_NEAREST, p=1.0),\n","            ], p=1),\n","\n","            A.HorizontalFlip(p=0.5),\n","            # A.VerticalFlip(p=0.5),\n","            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n","            # A.OneOf([\n","            #     A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n","            #     # A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=1.0),\n","            #     A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n","            # ], p=0.25),\n","            # A.CoarseDropout(max_holes=8, max_height=CFG.img_size[0]//20, max_width=CFG.img_size[1]//20,\n","            #                 min_holes=5, fill_value=0, mask_fill_value=0, p=0.5),\n","            ], p=1.0),\n","        \n","        \"valid_test\": A.Compose([\n","            A.Resize(*CFG.img_size, interpolation=cv2.INTER_NEAREST),\n","            ], p=1.0)\n","        }\n","    return data_transforms\n","\n","class build_dataset(Dataset):\n","    def __init__(self, df, label=True, transforms=None, cfg=None):\n","        self.df = df\n","        self.label = label\n","        self.img_paths = df['image_path'].tolist() # image\n","        self.ids = df['id'].tolist()\n","\n","        if 'mask_path' in df.columns:\n","            self.mask_paths  = df['mask_path'].tolist() # mask\n","        else:\n","            self.mask_paths = None\n","\n","        self.transforms = transforms\n","        self.n_25d_shift = cfg.n_25d_shift\n","\n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, index):\n","        #### load id\n","        id       = self.ids[index]\n","        #### load image\n","        img_path  = self.img_paths[index]\n","        img = self.load_2_5d_slice(img_path) # [h, w, c]\n","        h, w = img.shape[:2]\n","        \n","        if self.label: # train\n","            #### load mask\n","            mask_path = self.mask_paths[index]\n","            mask = np.load(mask_path).astype('float32')\n","            mask/=255.0 # scale mask to [0, 1]\n","\n","            ### augmentations\n","            data = self.transforms(image=img, mask=mask)\n","            img  = data['image']\n","            mask  = data['mask']\n","            img = np.transpose(img, (2, 0, 1)) # [h, w, c] => [c, h, w]\n","            mask = np.transpose(mask, (2, 0, 1)) # [h, w, c] => [c, h, w]\n","            return torch.tensor(img), torch.tensor(mask)\n","        \n","        else:  # test\n","            ### augmentations\n","            data = self.transforms(image=img)\n","            img  = data['image']\n","            img = np.transpose(img, (2, 0, 1)) # [h, w, c] => [c, h, w]\n","            return torch.tensor(img), id, h, w\n","\n","    ###############################################################\n","    ##### >>>>>>> trick: construct 2.5d slice images <<<<<<\n","    ###############################################################\n","    def load_2_5d_slice(self, middle_img_path):\n","        #### 步骤1: 获取中间图片的基本信息\n","        #### eg: middle_img_path: 'slice_0005_266_266_1.50_1.50.png' \n","        middle_slice_num = os.path.basename(middle_img_path).split('_')[1] # eg: 0005\n","        middle_str = 'slice_'+middle_slice_num\n","\n","        new_25d_imgs = []\n","\n","        ##### 步骤2：按照左右n_25d_shift数量进行填充，如果没有相应图片填充为Nan.\n","        ##### 注：经过EDA发现同一天的所有患者图片的shape是一致的\n","        for i in range(-self.n_25d_shift, self.n_25d_shift+1): # eg: i = {-2, -1, 0, 1, 2}\n","            shift_slice_num = int(middle_slice_num) + i\n","            shift_str = 'slice_'+str(shift_slice_num).zfill(4)\n","            shift_img_path = middle_img_path.replace(middle_str, shift_str)\n","            \n","            if os.path.exists(shift_img_path):\n","                shift_img = cv2.imread(shift_img_path, cv2.IMREAD_UNCHANGED) # [w, h]\n","                new_25d_imgs.append(shift_img)\n","            else:\n","                new_25d_imgs.append(None)\n","        \n","        ##### 步骤3：从中心开始往外循环，依次填补None的值\n","        ##### eg: n_25d_shift = 2, 那么形成5个channel, idx为[0, 1, 2, 3, 4], 所以依次处理的idx为[1, 3, 0, 4]\n","        shift_left_idxs = []\n","        shift_right_idxs = []\n","        for related_idx in range(self.n_25d_shift):\n","            shift_left_idxs.append(self.n_25d_shift - related_idx - 1)\n","            shift_right_idxs.append(self.n_25d_shift + related_idx + 1)\n","\n","        for left_idx, right_idx in zip(shift_left_idxs, shift_right_idxs):\n","            if new_25d_imgs[left_idx] is None:\n","                new_25d_imgs[left_idx] = new_25d_imgs[left_idx+1]\n","            if new_25d_imgs[right_idx] is None:\n","                new_25d_imgs[right_idx] = new_25d_imgs[right_idx-1]\n","\n","        new_25d_imgs = np.stack(new_25d_imgs, axis=2).astype('float32') # [w, h, c]\n","        mx_pixel = new_25d_imgs.max()\n","        if mx_pixel != 0:\n","            new_25d_imgs /= mx_pixel\n","        return new_25d_imgs\n","\n","def build_dataloader(df, fold, data_transforms, CFG):\n","    train_df = df.query(\"fold!=@fold\").reset_index(drop=True)\n","    valid_df = df.query(\"fold==@fold\").reset_index(drop=True)\n","    train_dataset = build_dataset(train_df, label=True, transforms=data_transforms['train'], cfg=CFG)\n","    valid_dataset = build_dataset(valid_df, label=True, transforms=data_transforms['valid_test'], cfg=CFG)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=CFG.train_bs, num_workers=CFG.num_worker, shuffle=True, pin_memory=True, drop_last=False)\n","    valid_loader = DataLoader(valid_dataset, batch_size=CFG.valid_bs, num_workers=CFG.num_worker, shuffle=False, pin_memory=True)\n","    \n","    return train_loader, valid_loader\n","\n","###############################################################\n","##### >>>>>>> part2: build_model <<<<<<\n","###############################################################\n","def build_model(CFG, test_flag=False):\n","    if test_flag:\n","        pretrain_weights = None\n","    else:\n","        pretrain_weights = \"imagenet\"\n","    # model = smp.UnetPlusPlus(\n","    model = smp.Unet(        \n","            encoder_name=CFG.backbone,\n","            encoder_weights=pretrain_weights, \n","            in_channels=2*CFG.n_25d_shift+1,             \n","            classes=CFG.num_classes,   \n","            activation=None,\n","        )\n","    model.to(CFG.device)\n","    return model\n","\n","###############################################################\n","##### >>>>>>> part3: build_loss <<<<<<\n","###############################################################\n","def batch_dice_loss(inputs, targets):\n","    inputs = inputs.sigmoid().flatten(1) # [b, c, w, h] => [b, c*w*h]\n","    targets = targets.flatten(1) # [b, c, w, h] => [b, c*w*h]\n","    numerator = 2 * (inputs * targets).sum(-1)\n","    denominator = inputs.sum(-1) + targets.sum(-1)\n","    loss = 1 - (numerator + 1) / (denominator + 1)\n","    return loss.sum()/loss.shape[0]\n","   \n","\n","def build_loss():\n","    # BCELoss     = smp.losses.SoftBCEWithLogitsLoss()\n","    # TverskyLoss = smp.losses.TverskyLoss(mode='multilabel', log_loss=False)\n","    # JaccardLoss = smp.losses.JaccardLoss(mode='multilabel')\n","    # DiceLoss    = smp.losses.DiceLoss(mode='multilabel')\n","    # LovaszLoss  = smp.losses.LovaszLoss(mode='multilabel', per_image=False)\n","    # TverskyLoss = smp.losses.TverskyLoss(mode='multilabel', log_loss=False)\n","    # FocalLoss    = smp.losses.FocalLoss(mode='multilabel')\n","    FocalLoss    = smp.losses.DiceLoss(mode='multilabel')\n","\n","      \n","    # return {\"BCELoss\":BCELoss, \"TverskyLoss\":TverskyLoss}\n","    # bce_loss = torch.nn.BCEWithLogitsLoss()\n","    # return {\"bce_loss\": bce_loss, \"dice_loss\":batch_dice_loss} \n","    return {\"FocalLoss\":FocalLoss}\n","\n","###############################################################\n","##### >>>>>>> part4: build_metric <<<<<<\n","###############################################################\n","def dice_coef(y_true, y_pred, thr=0.5, dim=(2,3), epsilon=0.001):\n","    y_true = y_true.to(torch.float32)\n","    y_pred = (y_pred>thr).to(torch.float32)\n","    inter = (y_true*y_pred).sum(dim=dim)\n","    den = y_true.sum(dim=dim) + y_pred.sum(dim=dim)\n","    dice = ((2*inter+epsilon)/(den+epsilon)).mean(dim=(1,0))\n","    return dice\n","\n","def iou_coef(y_true, y_pred, thr=0.5, dim=(2,3), epsilon=0.001):\n","    y_true = y_true.to(torch.float32)\n","    y_pred = (y_pred>thr).to(torch.float32)\n","    inter = (y_true*y_pred).sum(dim=dim)\n","    union = (y_true + y_pred - y_true*y_pred).sum(dim=dim)\n","    iou = ((inter+epsilon)/(union+epsilon)).mean(dim=(1,0))\n","    return iou\n","    \n","###############################################################\n","##### >>>>>>> part5: train & validation & test <<<<<<\n","###############################################################\n","def train_one_epoch(model, train_loader, optimizer, losses_dict, CFG):\n","    model.train()\n","    scaler = amp.GradScaler() \n","    # losses_all, bce_all, tverskly_all = 0, 0, 0\n","    # losses_all, bce_all, dice_all = 0, 0, 0\n","    losses_all, focal_all = 0, 0\n","\n","\n","    \n","    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc='Train ')\n","    for _, (images, masks) in pbar:\n","        optimizer.zero_grad()\n","\n","        images = images.to(CFG.device, dtype=torch.float) # [b, c, w, h]\n","        masks  = masks.to(CFG.device, dtype=torch.float)  # [b, c, w, h]\n","\n","        with amp.autocast(enabled=True):\n","            y_preds = model(images) # [b, c, w, h]\n","        \n","            # bce_loss = losses_dict[\"BCELoss\"](y_preds, masks)\n","            # tverskly_loss = losses_dict[\"TverskyLoss\"](y_preds, masks)\n","            # losses = bce_loss + tverskly_loss\n","\n","            # dic_loss = losses_dict[\"DiceLoss\"](y_preds, masks)\n","            # losses = dic_loss\n","\n","            # bce_loss = losses_dict[\"bce_loss\"](y_preds, masks)\n","            # dice_loss = losses_dict[\"dice_loss\"](y_preds, masks)\n","            # losses = bce_loss + dice_loss        \n","            focal_loss = losses_dict[\"FocalLoss\"](y_preds, masks)\n","            losses = focal_loss\n","\n","        scaler.scale(losses).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        \n","        # losses_all += losses.item() / images.shape[0]\n","        # bce_all += bce_loss.item() / images.shape[0]\n","        # tverskly_all += tverskly_loss.item() / images.shape[0]\n","        # losses_all += losses.item() / images.shape[0]\n","        # bce_all += bce_loss.item() / images.shape[0]\n","        # dice_all += dice_loss.item() / images.shape[0]  \n","        losses_all += losses.item() / images.shape[0]\n","        focal_all += focal_loss.item() / images.shape[0]              \n","    \n","    current_lr = optimizer.param_groups[0]['lr']\n","    print(\"lr: {:.4f}\".format(current_lr), flush=True)\n","    # print(\"loss: {:.3f}, bce_all: {:.3f}, tverskly_all: {:.3f}\".format(losses_all, bce_all, tverskly_all), flush=True)\n","    # print(\"loss: {:.3f}, dic_loss: {:.3f}\".format(losses_all, dic_loss), flush=True)\n","    # print(\"loss: {:.3f}, bce_all: {:.3f}, dice_all: {:.3f}\".format(losses_all, bce_all, dice_all), flush=True)\n","    print(\"loss: {:.3f}, focal_all: {:.3f}\".format(losses_all, focal_all), flush=True)\n","\n","    return losses_all\n","  \n","\n","def train_one_epoch1(model, train_loader, optimizer, losses_dict, CFG):\n","    # model.train()\n","    scaler = amp.GradScaler() \n","    # losses_all, bce_all, tverskly_all = 0, 0, 0\n","    # losses_all, bce_all, dice_all = 0, 0, 0\n","    losses_all, focal_all = 0, 0\n","\n","\n","    \n","    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc='Train ')\n","    for _, (images, masks) in pbar:\n","        optimizer.zero_grad()\n","\n","        images = images.to(CFG.device, dtype=torch.float) # [b, c, w, h]\n","        masks  = masks.to(CFG.device, dtype=torch.float)  # [b, c, w, h]\n","\n","        with amp.autocast(enabled=True):\n","            y_preds = model(images) # [b, c, w, h]\n","        \n","            # bce_loss = losses_dict[\"BCELoss\"](y_preds, masks)\n","            # tverskly_loss = losses_dict[\"TverskyLoss\"](y_preds, masks)\n","            # losses = bce_loss + tverskly_loss\n","\n","            # dic_loss = losses_dict[\"DiceLoss\"](y_preds, masks)\n","            # losses = dic_loss\n","\n","            # bce_loss = losses_dict[\"bce_loss\"](y_preds, masks)\n","            # dice_loss = losses_dict[\"dice_loss\"](y_preds, masks)\n","            # losses = bce_loss + dice_loss        \n","            focal_loss = losses_dict[\"FocalLoss\"](y_preds, masks)\n","            losses = focal_loss\n","\n","        scaler.scale(losses).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        \n","        # losses_all += losses.item() / images.shape[0]\n","        # bce_all += bce_loss.item() / images.shape[0]\n","        # tverskly_all += tverskly_loss.item() / images.shape[0]\n","        # losses_all += losses.item() / images.shape[0]\n","        # bce_all += bce_loss.item() / images.shape[0]\n","        # dice_all += dice_loss.item() / images.shape[0]  \n","        losses_all += losses.item() / images.shape[0]\n","        focal_all += focal_loss.item() / images.shape[0]              \n","    \n","    current_lr = optimizer.param_groups[0]['lr']\n","    print(\"lr: {:.4f}\".format(current_lr), flush=True)\n","    # print(\"loss: {:.3f}, bce_all: {:.3f}, tverskly_all: {:.3f}\".format(losses_all, bce_all, tverskly_all), flush=True)\n","    # print(\"loss: {:.3f}, dic_loss: {:.3f}\".format(losses_all, dic_loss), flush=True)\n","    # print(\"loss: {:.3f}, bce_all: {:.3f}, dice_all: {:.3f}\".format(losses_all, bce_all, dice_all), flush=True)\n","    print(\"loss: {:.3f}, focal_all: {:.3f}\".format(losses_all, focal_all), flush=True)\n","\n","    return losses_all    \n","        \n","@torch.no_grad()\n","def valid_one_epoch(model, valid_loader, CFG):\n","    model.eval()\n","    val_scores = []\n","    \n","    pbar = tqdm(enumerate(valid_loader), total=len(valid_loader), desc='Valid ')\n","    for _, (images, masks) in pbar:\n","        images  = images.to(CFG.device, dtype=torch.float) # [b, c, w, h]\n","        masks   = masks.to(CFG.device, dtype=torch.float)  # [b, c, w, h]\n","        \n","        y_preds = model(images) \n","        y_preds   = torch.nn.Sigmoid()(y_preds) # [b, c, w, h]\n","        \n","        val_dice = dice_coef(masks, y_preds).cpu().detach().numpy()\n","        val_jaccard = iou_coef(masks, y_preds).cpu().detach().numpy()\n","        val_scores.append([val_dice, val_jaccard])\n","        \n","    val_scores  = np.mean(val_scores, axis=0)\n","    val_dice, val_jaccard = val_scores\n","    print(\"val_dice: {:.4f}, val_jaccard: {:.4f}\".format(val_dice, val_jaccard), flush=True)\n","    \n","    return val_dice, val_jaccard\n","\n","@torch.no_grad()\n","def test_one_epoch(ckpt_paths, test_loader, CFG):\n","    pred_strings = []\n","    pred_ids = []\n","    pred_classes = []\n","    \n","    pbar = tqdm(enumerate(test_loader), total=len(test_loader), desc='Test: ')\n","    for _, (images, ids, h, w) in pbar:\n","\n","        images  = images.to(CFG.device, dtype=torch.float) # [b, c, w, h]\n","        size = images.size()\n","        masks = torch.zeros((size[0], 3, size[2], size[3]), device=CFG.device, dtype=torch.float32) # [b, c, w, h]\n","        \n","        ############################################\n","        ##### >>>>>>> cross validation infer <<<<<<\n","        ############################################\n","        for sub_ckpt_path in ckpt_paths:\n","            model = build_model(CFG, test_flag=True)\n","            model.load_state_dict(torch.load(sub_ckpt_path))\n","            model.eval()\n","            y_preds = model(images) # [b, c, w, h]\n","            y_preds   = torch.nn.Sigmoid()(y_preds)\n","            masks += y_preds/len(ckpt_paths)\n","        \n","        masks = (masks.permute((0, 2, 3, 1))>CFG.thr).to(torch.uint8).cpu().detach().numpy() # [n, h, w, c]\n","        result = masks2rles(masks, ids, h, w)\n","        pred_strings.extend(result[0])\n","        pred_ids.extend(result[1])\n","        pred_classes.extend(result[2])\n","    return pred_strings, pred_ids, pred_classes\n","\n","class LearningRateDecay():\n","    def plot(self, epochs, title=\"LR Schedule\"):\n","        rates = [self(i) for i in epochs]\n","        plt.style.use(\"ggplot\")\n","        plt.figure()\n","        plt.plot(epochs, rates)\n","        plt.xlabel(\"Epochs\")\n","        plt.ylabel(\"Rates\")\n","        plt.title(title)  \n","\n","class PolynomialDecay(LearningRateDecay):\n","    def __init__(self, max_epochs=100, initial_alpha=0.01, power=1.0):\n","        self.max_epochs = max_epochs\n","        self.initial_alpha = initial_alpha\n","        self.power = power\n","\n","    def __call__(self, epoch):\n","        decay = (1 - (epoch / float(self.max_epochs))) ** self.power\n","        new_alpha = self.initial_alpha * decay\n","\n","        return float(new_alpha)\n","\n","      \n","\n","def make_callbacks(best_model_file_name):\n","    cb_monitor = 'val_loss'\n","    cb_mode = 'min'\n","    cb_verbose = 1\n","\n","    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n","        best_model_file_name, save_best_only=True,\n","        save_weights_only=False, monitor=cb_monitor, mode=cb_mode,\n","        verbose=cb_verbose)\n","    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n","    \n","    return [checkpoint, lr_callback]\n","\n","def adjust_learning_rate_poly(optimizer, initial_lr, iteration, max_iter):\n","    \"\"\"Sets the learning rate\n","    # Adapted from PyTorch Imagenet example:\n","    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n","    \"\"\"\n","    lr = initial_lr * ( 1 - (iteration / max_iter)) * ( 1 - (iteration / max_iter))\n","    if ( lr < 1.0e-7 ):\n","      lr = 1.0e-7\n","\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","    return lr\n","\n","if __name__ == '__main__':\n","    ###############################################################\n","    ##### >>>>>>> config <<<<<<\n","    ###############################################################\n","    class CFG:\n","        # step1: hyper-parameter\n","        seed = 402  # birthday\n","        num_worker = 8 # debug => 0\n","        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        ckpt_fold = \"ckpt-frank\"\n","        # ckpt_name = \"efficientnetb1_img224224_bs128_fold4_2.5d\"\n","        ckpt_name = \"eb2_img320320_bs32_fold01_poly\"\n","        # ckpt_name = \"efficientnetb2_img256256_bs64_fold1_2.5d_dice_wd1e6\"\n","        \n","        # step2: data\n","        n_25d_shift = 2\n","        n_fold = 2\n","        # img_size = [224, 224]\n","        # img_size = [256, 256]\n","        img_size = [320, 320]\n","        train_bs = 32#128\n","        valid_bs = train_bs * 2\n","        # step3: model\n","        backbone = 'efficientnet-b2'\n","        num_classes = 3\n","\n","        # step4: optimizer\n","        epoch = 1#20#12\n","        lr = 1e-3#1e-3\n","        wd = 1e-5#1e-5\n","        lr_drop = 20\n","\n","        # step5: infer\n","        thr = 0.45\n","        trn_fold=[0]#[0,1,2,3,4]\n","\n","    \n","    set_seed(CFG.seed)\n","    ckpt_path = f\"/content/drive/MyDrive/uw-madison/input/{CFG.ckpt_fold}/{CFG.ckpt_name}\"\n","    if not os.path.exists(ckpt_path):\n","        os.makedirs(ckpt_path)\n","\n","    train_val_flag = True\n","    if train_val_flag:\n","        ###############################################################\n","        ##### part0: data preprocess\n","        ###############################################################\n","        # document: https://pandas.pydata.org/docs/reference/frame.html\n","        # df = pd.read_csv('/content/drive/MyDrive/uw-madison/input/uwmgi-mask-dataset/train.csv')\n","        df = pd.read_csv('/content/drive/MyDrive/uw-madison/input/tmp/train.csv')\n","\n","        #Remove Faulty\n","        fault1 = 'case7_day0'\n","        fault2 = 'case81_day30'\n","        df = df[~df['id'].str.contains(fault1) & ~df['id'].str.contains(fault2)].reset_index(drop=True)\n","        df['segmentation'] = df.segmentation.fillna('') # .fillna(): 填充NaN的值为空\n","        # rle mask length\n","        df['rle_len'] = df.segmentation.map(len) # .map(): 特定列中的每一个元素应用一个函数len\n","        # image/mask path\n","        # df['image_path'] = '/content/drive/MyDrive/uw-madison/input/tmp/images/' + df['id'].astype(str) + '.npy'\n","        # df['mask_path'] = '/content/drive/MyDrive/uw-madison/input/tmp/masks/' + df['id'].astype(str) + '.npy' \n","\n","        # df['image_path'] = df.image_path.str.replace('/kaggle/','/content/drive/MyDrive/uw-madison/') # .str: 特定列应用python字符串处理方法 \n","        # df['mask_path'] = df.mask_path.str.replace('/kaggle/','/content/drive/MyDrive/uw-madison/')\n","        # df['mask_path'] = df.mask_path.str.replace('/png/','/np').str.replace('.png','.npy')\n","        # rle list of each id\n","        df2 = df.groupby(['id'])['segmentation'].agg(list).to_frame().reset_index() # .grouby(): 特定列划分group.\n","        # total length of all rles of each id\n","        df2 = df2.merge(df.groupby(['id'])['rle_len'].agg(sum).to_frame().reset_index()) # .agg(): 特定列应用operations\n","        # df = df.drop(columns=['segmentation', 'class', 'rle_len']) # .drop(): 特定列的删除\n","        df = df.drop(columns=['segmentation', 'rle_len']) # .drop(): 特定列的删除\n","\n","        df = df.groupby(['id']).head(1).reset_index(drop=True)\n","        # empty mask\n","        df = df.merge(df2, on=['id']) # .merge(): 特定列的合并\n","        df['empty'] = (df.rle_len==0) \n","\n","        ###############################################################\n","        ##### >>>>>>> trick1: cross validation train <<<<<<\n","        ###############################################################\n","        # document: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedGroupKFold.html\n","        skf = StratifiedGroupKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n","        for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['empty'], groups = df[\"case\"])):\n","            df.loc[val_idx, 'fold'] = fold\n","        \n","        # for fold in range(CFG.n_fold):\n","        for fold in CFG.trn_fold:\n","\n","            print(f'#'*80, flush=True)\n","            print(f'###### Fold: {fold}', flush=True)\n","            print(f'#'*80, flush=True)\n","\n","            ###############################################################\n","            ##### >>>>>>> step2: combination <<<<<<\n","            ##### build_transforme() & build_dataset() & build_dataloader()\n","            ##### build_model() & build_loss()\n","            ###############################################################\n","            data_transforms = build_transforms(CFG)  \n","            train_loader, valid_loader = build_dataloader(df, fold, data_transforms, CFG) # dataset & dtaloader\n","            model = build_model(CFG) # model\n","            optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.wd)\n","            lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, CFG.lr_drop, 0.5) \n","            losses_dict = build_loss() # loss\n","\n","            best_val_dice = 0\n","            best_epoch = 0\n","            best_loss = 1000\n","            n=2\n","\n","\n","            # model.compile(optimizer=optimizer, loss=losses_dict[\"FocalLoss\"], metrics=[dice_coef,iou_coef])\n","            # schedule = PolynomialDecay(max_epochs=CFG.epoch, initial_alpha=1e-1, power=5)\n","            # callbacks = [LearningRateScheduler(schedule)]\n","            # save_path = f\"{ckpt_path}/best_fold{fold}.pth\"\n","            # callbacks = make_callbacks(save_path)\n","            # model.fit(\n","            #     train_loader, \n","            #     validation_data=valid_loader, \n","            #     epochs=CFG.epoch, \n","            #     verbose=1,\n","            #     callbacks=callbacks\n","            #     )\n","            \n","            for epoch in range(1, CFG.epoch+1):\n","                start_time = time.time()\n","\n","                lr = adjust_learning_rate_poly(optimizer, CFG.lr, epoch, CFG.epoch)\n","                print(\"lr: {:.4f}\".format(lr), flush=True)\n","\n","\n","\n","\n","                # model.train()\n","\n","                current_loss = train_one_epoch(model, train_loader, optimizer, losses_dict, CFG)\n","                lr_scheduler.step()\n","                val_dice, val_jaccard = valid_one_epoch(model, valid_loader, CFG)                \n","\n","                ###############################################################\n","                ##### >>>>>>> step3: train & val <<<<<<\n","                ###############################################################\n","                # if epoch % n == 1 or epoch==CFG.epoch:\n","                #   current_loss = train_one_epoch1(model, train_loader, optimizer, losses_dict, CFG)\n","                #   lr_scheduler.step()\n","                #   val_dice, val_jaccard = valid_one_epoch(model, valid_loader, CFG)\n","                \n","                ###############################################################\n","                ##### >>>>>>> step4: save best model <<<<<<\n","                ###############################################################\n","                # is_best = (val_dice > best_val_dice)\n","                is_best = (current_loss <= best_loss)\n","\n","                # best_val_dice = max(best_val_dice, val_dice)\n","                best_loss = min(best_loss, current_loss)\n","\n","                # save_path = f\"{ckpt_path}/best_fold{fold}.pth\"\n","                # if os.path.isfile(save_path):\n","                #     os.remove(save_path) \n","                # torch.save(model.state_dict(), save_path)\n","\n","                if is_best:\n","                    save_path = f\"{ckpt_path}/best_fold{fold}.pth\"\n","                    if os.path.isfile(save_path):\n","                        os.remove(save_path) \n","                    torch.save(model.state_dict(), save_path)\n","                \n","                epoch_time = time.time() - start_time\n","                print(\"epoch:{}, time:{:.2f}s, best:{:.4f}, best_loss:{:.3f}\\n\".format(epoch, epoch_time, val_dice, best_loss), flush=True)\n","\n","\n","                \n","    test_flag = False\n","    if test_flag:\n","        set_seed(CFG.seed)\n","        ###############################################################\n","        ##### part0: data preprocess\n","        ###############################################################\n","        sub_df = pd.read_csv('/content/drive/MyDrive/uw-madison/input/uw-madison-gi-tract-image-segmentation/sample_submission.csv')\n","        if not len(sub_df):\n","            sub_firset = True\n","            sub_df = pd.read_csv('/content/drive/MyDrive/uw-madison/input/uw-madison-gi-tract-image-segmentation/train.csv')[:1000*3]\n","            sub_df = sub_df.drop(columns=['class','segmentation']).drop_duplicates()\n","            paths = glob(f'/content/drive/MyDrive/uw-madison/input/uw-madison-gi-tract-image-segmentation/train/**/*png',recursive=True)\n","        else:\n","            sub_firset = False\n","            sub_df = sub_df.drop(columns=['class','predicted']).drop_duplicates()\n","            paths = glob(f'/content/drive/MyDrive/uw-madison/input/uw-madison-gi-tract-image-segmentation/test/**/*png',recursive=True)\n","        sub_df = sub_df.apply(get_metadata,axis=1)\n","        path_df = pd.DataFrame(paths, columns=['image_path'])\n","        path_df = path_df.apply(path2info, axis=1)\n","        test_df = sub_df.merge(path_df, on=['case','day','slice'], how='left')\n","\n","        data_transforms = build_transforms(CFG)\n","        test_dataset = build_dataset(test_df, label=False, transforms=data_transforms['valid_test'], cfg=CFG)\n","        test_loader  = DataLoader(test_dataset, batch_size=CFG.valid_bs, num_workers=2, shuffle=False, pin_memory=False)\n","\n","        ###############################################################\n","        ##### >>>>>>> step2: infer <<<<<<\n","        ###############################################################\n","        # attention: change the corresponding upload path to kaggle.\n","        ckpt_paths  = glob(f'{ckpt_path}/best*')\n","        assert len(ckpt_paths) == CFG.n_fold, \"ckpt path error!\"\n","\n","        pred_strings, pred_ids, pred_classes = test_one_epoch(ckpt_paths, test_loader, CFG)\n","\n","        ###############################################################\n","        ##### step3: submit\n","        ###############################################################\n","        pred_df = pd.DataFrame({\n","            \"id\":pred_ids,\n","            \"class\":pred_classes,\n","            \"predicted\":pred_strings\n","        })\n","        if not sub_firset:\n","            sub_df = pd.read_csv('/content/drive/MyDrive/uw-madison/input/uw-madison-gi-tract-image-segmentation/sample_submission.csv')\n","            del sub_df['predicted']\n","        else:\n","            sub_df = pd.read_csv('/content/drive/MyDrive/uw-madison/input/uw-madison-gi-tract-image-segmentation/train.csv')[:1000*3]\n","            del sub_df['segmentation']\n","            \n","        sub_df = sub_df.merge(pred_df, on=['id','class'])\n","        sub_df.to_csv('submission.csv',index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ns6zuK3FKwHL"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3uLk1nVD7Nff"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GqNDs_HJyZZ-"},"outputs":[],"source":["# !export LD_LIBRARY_PATH\n","# !find -name libnvrtc.so.11.2\n","# !ls /usr/local/cuda-11.0/lib64/libnvrtc.so.11*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xlNF8vCHn5R0"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Hd9s6tCodki"},"outputs":[],"source":["# !ls /content/drive/MyDrive/uw-madison/input/uwmgi-mask-dataset/np/uw-madison-gi-tract-image-segmentation/train/case43/case43_day0/scans/slice_0138_266_266_1.50_1.50.npy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRFFv2AUveag"},"outputs":[],"source":["# ! pip install kaggle\n","# ! mkdir ~/.kaggle\n","# ! cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n","# ! chmod 600 ~/.kaggle/kaggle.json\n","# !kaggle kernels pull skyscreen/uw-efficientnetb2-img320384-bs128-fold4-2-5d-infer/pl_ckpt_path\n","# !kaggle datasets download -d awsaf49/uwmgi-mask-dataset\n","# !unzip -o /content/uwmgi-mask-dataset.zip  -d /content/drive/MyDrive/uw-madison/input/uwmgi-mask-dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_qvSEHRWU4du"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gczoYU8I95m5"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","# !find  /usr/local/ -name \"libnvrtc.so*\"\n","!nvcc --version"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"eb2_img384384_bs32_fold1_batchloss.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
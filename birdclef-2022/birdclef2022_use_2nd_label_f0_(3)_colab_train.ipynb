{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":881,"status":"ok","timestamp":1653294208512,"user":{"displayName":"darwin zhao","userId":"12087337285275609974"},"user_tz":-480},"id":"bsfDhhVsez-P","outputId":"33073439-60df-4418-f453-ef0278d0e507"},"outputs":[{"name":"stdout","output_type":"stream","text":["opencv-contrib-python         4.1.2.30\n","opencv-python                 4.1.2.30\n"]}],"source":["!pip list |grep opencv"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36998,"status":"ok","timestamp":1653294245507,"user":{"displayName":"darwin zhao","userId":"12087337285275609974"},"user_tz":-480},"id":"zed6x8VjxLNx","outputId":"11700452-47bd-4bd4-e639-7429202b6448"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting timm\n","  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n","\u001b[?25l\r\u001b[K     |▊                               | 10 kB 34.3 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 235 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 245 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 307 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 358 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 368 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 419 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 431 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: torch\u003e=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.11.0+cu113)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.12.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch\u003e=1.4-\u003etimm) (4.2.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision-\u003etimm) (2.23.0)\n","Requirement already satisfied: pillow!=8.3.*,\u003e=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision-\u003etimm) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision-\u003etimm) (1.21.6)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etorchvision-\u003etimm) (1.24.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etorchvision-\u003etimm) (2021.10.8)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etorchvision-\u003etimm) (3.0.4)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etorchvision-\u003etimm) (2.10)\n","Installing collected packages: timm\n","Successfully installed timm-0.5.4\n","Collecting transformers\n","  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n","\u001b[K     |████████████████████████████████| 4.2 MB 5.4 MB/s \n","\u001b[?25hCollecting huggingface-hub\u003c1.0,\u003e=0.1.0\n","  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n","\u001b[K     |████████████████████████████████| 84 kB 3.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting tokenizers!=0.11.3,\u003c0.13,\u003e=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 61.7 MB/s \n","\u001b[?25hCollecting pyyaml\u003e=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 68.5 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.1.0-\u003etransformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003etransformers) (3.0.9)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003etransformers) (3.8.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2021.10.8)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (1.24.3)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2.10)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.6.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n","Collecting git+https://github.com/albu/albumentations\n","  Cloning https://github.com/albu/albumentations to /tmp/pip-req-build-p5ur3sd0\n","  Running command git clone -q https://github.com/albu/albumentations /tmp/pip-req-build-p5ur3sd0\n","Requirement already satisfied: numpy\u003e=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (1.4.1)\n","Requirement already satisfied: scikit-image\u003c0.19,\u003e=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (0.18.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (6.0)\n","Collecting qudida\u003e=0.0.4\n","  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n","Requirement already satisfied: opencv-python\u003e=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (4.1.2.30)\n","Collecting opencv-python-headless\u003e=4.0.1\n","  Downloading opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.8 MB)\n","\u001b[K     |████████████████████████████████| 47.8 MB 144.5 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn\u003e=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida\u003e=0.0.4-\u003ealbumentations==1.1.0) (1.0.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida\u003e=0.0.4-\u003ealbumentations==1.1.0) (4.2.0)\n","Requirement already satisfied: PyWavelets\u003e=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image\u003c0.19,\u003e=0.16.1-\u003ealbumentations==1.1.0) (1.3.0)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,\u003e=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image\u003c0.19,\u003e=0.16.1-\u003ealbumentations==1.1.0) (7.1.2)\n","Requirement already satisfied: matplotlib!=3.0.0,\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image\u003c0.19,\u003e=0.16.1-\u003ealbumentations==1.1.0) (3.2.2)\n","Requirement already satisfied: tifffile\u003e=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image\u003c0.19,\u003e=0.16.1-\u003ealbumentations==1.1.0) (2021.11.2)\n","Requirement already satisfied: imageio\u003e=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image\u003c0.19,\u003e=0.16.1-\u003ealbumentations==1.1.0) (2.4.1)\n","Requirement already satisfied: networkx\u003e=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image\u003c0.19,\u003e=0.16.1-\u003ealbumentations==1.1.0) (2.6.3)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,\u003e=2.0.0-\u003escikit-image\u003c0.19,\u003e=0.16.1-\u003ealbumentations==1.1.0) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,\u003e=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,\u003e=2.0.0-\u003escikit-image\u003c0.19,\u003e=0.16.1-\u003ealbumentations==1.1.0) (3.0.9)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,\u003e=2.0.0-\u003escikit-image\u003c0.19,\u003e=0.16.1-\u003ealbumentations==1.1.0) (1.4.2)\n","Requirement already satisfied: python-dateutil\u003e=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,\u003e=2.0.0-\u003escikit-image\u003c0.19,\u003e=0.16.1-\u003ealbumentations==1.1.0) (2.8.2)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil\u003e=2.1-\u003ematplotlib!=3.0.0,\u003e=2.0.0-\u003escikit-image\u003c0.19,\u003e=0.16.1-\u003ealbumentations==1.1.0) (1.15.0)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn\u003e=0.19.1-\u003equdida\u003e=0.0.4-\u003ealbumentations==1.1.0) (3.1.0)\n","Requirement already satisfied: joblib\u003e=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn\u003e=0.19.1-\u003equdida\u003e=0.0.4-\u003ealbumentations==1.1.0) (1.1.0)\n","Building wheels for collected packages: albumentations\n","  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for albumentations: filename=albumentations-1.1.0-py3-none-any.whl size=112723 sha256=8971a6e71a0b7dab8433b8e35bb6f785cc7abd9bbe6c3771b42f79a08cb28a8c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-769h43gb/wheels/63/11/1a/c77caf3ae9b9b6d57b3ee5e6a41a50f3bc12c66a70f6b90bf0\n","Successfully built albumentations\n","Installing collected packages: opencv-python-headless, qudida, albumentations\n","  Attempting uninstall: albumentations\n","    Found existing installation: albumentations 0.1.12\n","    Uninstalling albumentations-0.1.12:\n","      Successfully uninstalled albumentations-0.1.12\n","Successfully installed albumentations-1.1.0 opencv-python-headless-4.5.5.64 qudida-0.0.4\n","\u001b[31mERROR: Could not find a version that satisfies the requirement cv2 (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for cv2\u001b[0m\n","\u001b[31mERROR: Invalid requirement: 'opencv-python-headless=4.5.5.64'\n","Hint: = is not a valid operator. Did you mean == ?\u001b[0m\n","Collecting opencv-python-headless==4.5.2.52\n","  Downloading opencv_python_headless-4.5.2.52-cp37-cp37m-manylinux2014_x86_64.whl (38.2 MB)\n","\u001b[K     |████████████████████████████████| 38.2 MB 1.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless==4.5.2.52) (1.21.6)\n","Installing collected packages: opencv-python-headless\n","  Attempting uninstall: opencv-python-headless\n","    Found existing installation: opencv-python-headless 4.5.5.64\n","    Uninstalling opencv-python-headless-4.5.5.64:\n","      Successfully uninstalled opencv-python-headless-4.5.5.64\n","Successfully installed opencv-python-headless-4.5.2.52\n","Requirement already satisfied: opencv-python==4.1.2.30 in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n","Requirement already satisfied: numpy\u003e=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==4.1.2.30) (1.21.6)\n","Collecting torchlibrosa\n","  Downloading torchlibrosa-0.0.9-py3-none-any.whl (11 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchlibrosa) (1.21.6)\n","Requirement already satisfied: librosa\u003e=0.6.0 in /usr/local/lib/python3.7/dist-packages (from torchlibrosa) (0.8.1)\n","Requirement already satisfied: scipy\u003e=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa\u003e=0.6.0-\u003etorchlibrosa) (1.4.1)\n","Requirement already satisfied: soundfile\u003e=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa\u003e=0.6.0-\u003etorchlibrosa) (0.10.3.post1)\n","Requirement already satisfied: decorator\u003e=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa\u003e=0.6.0-\u003etorchlibrosa) (4.4.2)\n","Requirement already satisfied: scikit-learn!=0.19.0,\u003e=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa\u003e=0.6.0-\u003etorchlibrosa) (1.0.2)\n","Requirement already satisfied: pooch\u003e=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa\u003e=0.6.0-\u003etorchlibrosa) (1.6.0)\n","Requirement already satisfied: resampy\u003e=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa\u003e=0.6.0-\u003etorchlibrosa) (0.2.2)\n","Requirement already satisfied: numba\u003e=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa\u003e=0.6.0-\u003etorchlibrosa) (0.51.2)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa\u003e=0.6.0-\u003etorchlibrosa) (21.3)\n","Requirement already satisfied: audioread\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa\u003e=0.6.0-\u003etorchlibrosa) (2.1.9)\n","Requirement already satisfied: joblib\u003e=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa\u003e=0.6.0-\u003etorchlibrosa) (1.1.0)\n","Requirement already satisfied: llvmlite\u003c0.35,\u003e=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba\u003e=0.43.0-\u003elibrosa\u003e=0.6.0-\u003etorchlibrosa) (0.34.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba\u003e=0.43.0-\u003elibrosa\u003e=0.6.0-\u003etorchlibrosa) (57.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003elibrosa\u003e=0.6.0-\u003etorchlibrosa) (3.0.9)\n","Requirement already satisfied: requests\u003e=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch\u003e=1.0-\u003elibrosa\u003e=0.6.0-\u003etorchlibrosa) (2.23.0)\n","Requirement already satisfied: appdirs\u003e=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch\u003e=1.0-\u003elibrosa\u003e=0.6.0-\u003etorchlibrosa) (1.4.4)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.19.0-\u003epooch\u003e=1.0-\u003elibrosa\u003e=0.6.0-\u003etorchlibrosa) (3.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.19.0-\u003epooch\u003e=1.0-\u003elibrosa\u003e=0.6.0-\u003etorchlibrosa) (2021.10.8)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.19.0-\u003epooch\u003e=1.0-\u003elibrosa\u003e=0.6.0-\u003etorchlibrosa) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.19.0-\u003epooch\u003e=1.0-\u003elibrosa\u003e=0.6.0-\u003etorchlibrosa) (1.24.3)\n","Requirement already satisfied: six\u003e=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy\u003e=0.2.2-\u003elibrosa\u003e=0.6.0-\u003etorchlibrosa) (1.15.0)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,\u003e=0.14.0-\u003elibrosa\u003e=0.6.0-\u003etorchlibrosa) (3.1.0)\n","Requirement already satisfied: cffi\u003e=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile\u003e=0.10.2-\u003elibrosa\u003e=0.6.0-\u003etorchlibrosa) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi\u003e=1.0-\u003esoundfile\u003e=0.10.2-\u003elibrosa\u003e=0.6.0-\u003etorchlibrosa) (2.21)\n","Installing collected packages: torchlibrosa\n","Successfully installed torchlibrosa-0.0.9\n"]}],"source":["!pip install timm\n","!pip install transformers\n","!pip install -U git+https://github.com/albu/albumentations --no-cache-dir\n","!pip install cv2\n","!pip uninstall opencv-python-headless=4.5.5.64\n","!pip install opencv-python-headless==4.5.2.52\n","\n","!pip install opencv-python==4.1.2.30\n","!pip install torchlibrosa\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r7DESGRr0W9Q"},"outputs":[],"source":["!pip install colorednoise \u003e /dev/null"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3Hv-Pi90W9R"},"outputs":[],"source":["# fold 0 test run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P0W7tuKI0W9T"},"outputs":[],"source":["import cv2\n","import audioread\n","import logging\n","import gc\n","import os\n","import sys\n","sys.path.append('/content/drive/MyDrive/birdclef2022/pytorch-image-models-masterr')\n","import random\n","import time\n","import warnings\n","\n","import librosa\n","import colorednoise as cn\n","import numpy as np\n","import pandas as pd\n","import soundfile as sf\n","import timm\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.data as torchdata\n","\n","from contextlib import contextmanager\n","from joblib import Parallel, delayed\n","from pathlib import Path\n","from typing import Optional\n","from sklearn.model_selection import StratifiedKFold, GroupKFold\n","from sklearn import metrics\n","from sklearn.metrics import mean_squared_error, roc_auc_score\n","\n","from albumentations.core.transforms_interface import ImageOnlyTransform\n","from torchlibrosa.stft import LogmelFilterBank, Spectrogram\n","from torchlibrosa.augmentation import SpecAugmentation\n","from tqdm import tqdm\n","\n","import albumentations as A\n","import albumentations.pytorch.transforms as T\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z7I0Zh4-eyRX"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2pUnWrGQ-NIY"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNhqADVH0W9U"},"outputs":[],"source":["import transformers\n","from torch.cuda.amp import autocast, GradScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mM57g_c80W9V","outputId":"e4596c93-8bdd-4b0d-d468-249ab550367b"},"outputs":[{"data":{"text/plain":["14852"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["import glob\n","\n","all_path = glob.glob('/content/drive/MyDrive/birdclef2022/birdclef2022-audio-to-numpy-1-4/train_np/*/*.npy')\\\n","+ glob.glob('/content/drive/MyDrive/birdclef2022/birdclef2022-audio-to-numpy-2-4/train_np/*/*.npy')\\\n","+ glob.glob('/content/drive/MyDrive/birdclef2022/birdclef2022-audio-to-numpy-3-4/train_np/*/*.npy')\\\n","+ glob.glob('/content/drive/MyDrive/birdclef2022/birdclef2022-audio-to-numpy-4-4/train_np/*/*.npy')\n","\n","len(all_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"w6EXQL0M0W9W","outputId":"3dd9e011-05bc-4eec-ef29-38a51e5e7d2b"},"outputs":[{"data":{"text/html":["\n","  \u003cdiv id=\"df-d0ab5672-8760-496f-9531-8dc869f1b372\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eprimary_label\u003c/th\u003e\n","      \u003cth\u003esecondary_labels\u003c/th\u003e\n","      \u003cth\u003etype\u003c/th\u003e\n","      \u003cth\u003elatitude\u003c/th\u003e\n","      \u003cth\u003elongitude\u003c/th\u003e\n","      \u003cth\u003escientific_name\u003c/th\u003e\n","      \u003cth\u003ecommon_name\u003c/th\u003e\n","      \u003cth\u003eauthor\u003c/th\u003e\n","      \u003cth\u003elicense\u003c/th\u003e\n","      \u003cth\u003erating\u003c/th\u003e\n","      \u003cth\u003etime\u003c/th\u003e\n","      \u003cth\u003eurl\u003c/th\u003e\n","      \u003cth\u003efilename\u003c/th\u003e\n","      \u003cth\u003enew_target\u003c/th\u003e\n","      \u003cth\u003elen_new_target\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003eafrsil1\u003c/td\u003e\n","      \u003ctd\u003e[]\u003c/td\u003e\n","      \u003ctd\u003e['call', 'flight call']\u003c/td\u003e\n","      \u003ctd\u003e12.3910\u003c/td\u003e\n","      \u003ctd\u003e-1.4930\u003c/td\u003e\n","      \u003ctd\u003eEuodice cantans\u003c/td\u003e\n","      \u003ctd\u003eAfrican Silverbill\u003c/td\u003e\n","      \u003ctd\u003eBram Piot\u003c/td\u003e\n","      \u003ctd\u003eCreative Commons Attribution-NonCommercial-Sha...\u003c/td\u003e\n","      \u003ctd\u003e2.5\u003c/td\u003e\n","      \u003ctd\u003e08:00\u003c/td\u003e\n","      \u003ctd\u003ehttps://www.xeno-canto.org/125458\u003c/td\u003e\n","      \u003ctd\u003eafrsil1/XC125458.ogg\u003c/td\u003e\n","      \u003ctd\u003eafrsil1\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eafrsil1\u003c/td\u003e\n","      \u003ctd\u003e['houspa', 'redava', 'zebdov']\u003c/td\u003e\n","      \u003ctd\u003e['call']\u003c/td\u003e\n","      \u003ctd\u003e19.8801\u003c/td\u003e\n","      \u003ctd\u003e-155.7254\u003c/td\u003e\n","      \u003ctd\u003eEuodice cantans\u003c/td\u003e\n","      \u003ctd\u003eAfrican Silverbill\u003c/td\u003e\n","      \u003ctd\u003eDan Lane\u003c/td\u003e\n","      \u003ctd\u003eCreative Commons Attribution-NonCommercial-Sha...\u003c/td\u003e\n","      \u003ctd\u003e3.5\u003c/td\u003e\n","      \u003ctd\u003e08:30\u003c/td\u003e\n","      \u003ctd\u003ehttps://www.xeno-canto.org/175522\u003c/td\u003e\n","      \u003ctd\u003eafrsil1/XC175522.ogg\u003c/td\u003e\n","      \u003ctd\u003eafrsil1 houspa redava zebdov\u003c/td\u003e\n","      \u003ctd\u003e4\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003eafrsil1\u003c/td\u003e\n","      \u003ctd\u003e[]\u003c/td\u003e\n","      \u003ctd\u003e['call', 'song']\u003c/td\u003e\n","      \u003ctd\u003e16.2901\u003c/td\u003e\n","      \u003ctd\u003e-16.0321\u003c/td\u003e\n","      \u003ctd\u003eEuodice cantans\u003c/td\u003e\n","      \u003ctd\u003eAfrican Silverbill\u003c/td\u003e\n","      \u003ctd\u003eBram Piot\u003c/td\u003e\n","      \u003ctd\u003eCreative Commons Attribution-NonCommercial-Sha...\u003c/td\u003e\n","      \u003ctd\u003e4.0\u003c/td\u003e\n","      \u003ctd\u003e11:30\u003c/td\u003e\n","      \u003ctd\u003ehttps://www.xeno-canto.org/177993\u003c/td\u003e\n","      \u003ctd\u003eafrsil1/XC177993.ogg\u003c/td\u003e\n","      \u003ctd\u003eafrsil1\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003eafrsil1\u003c/td\u003e\n","      \u003ctd\u003e[]\u003c/td\u003e\n","      \u003ctd\u003e['alarm call', 'call']\u003c/td\u003e\n","      \u003ctd\u003e17.0922\u003c/td\u003e\n","      \u003ctd\u003e54.2958\u003c/td\u003e\n","      \u003ctd\u003eEuodice cantans\u003c/td\u003e\n","      \u003ctd\u003eAfrican Silverbill\u003c/td\u003e\n","      \u003ctd\u003eOscar Campbell\u003c/td\u003e\n","      \u003ctd\u003eCreative Commons Attribution-NonCommercial-Sha...\u003c/td\u003e\n","      \u003ctd\u003e4.0\u003c/td\u003e\n","      \u003ctd\u003e11:00\u003c/td\u003e\n","      \u003ctd\u003ehttps://www.xeno-canto.org/205893\u003c/td\u003e\n","      \u003ctd\u003eafrsil1/XC205893.ogg\u003c/td\u003e\n","      \u003ctd\u003eafrsil1\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003eafrsil1\u003c/td\u003e\n","      \u003ctd\u003e[]\u003c/td\u003e\n","      \u003ctd\u003e['flight call']\u003c/td\u003e\n","      \u003ctd\u003e21.4581\u003c/td\u003e\n","      \u003ctd\u003e-157.7252\u003c/td\u003e\n","      \u003ctd\u003eEuodice cantans\u003c/td\u003e\n","      \u003ctd\u003eAfrican Silverbill\u003c/td\u003e\n","      \u003ctd\u003eRoss Gallardy\u003c/td\u003e\n","      \u003ctd\u003eCreative Commons Attribution-NonCommercial-Sha...\u003c/td\u003e\n","      \u003ctd\u003e3.0\u003c/td\u003e\n","      \u003ctd\u003e16:30\u003c/td\u003e\n","      \u003ctd\u003ehttps://www.xeno-canto.org/207431\u003c/td\u003e\n","      \u003ctd\u003eafrsil1/XC207431.ogg\u003c/td\u003e\n","      \u003ctd\u003eafrsil1\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d0ab5672-8760-496f-9531-8dc869f1b372')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-d0ab5672-8760-496f-9531-8dc869f1b372 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d0ab5672-8760-496f-9531-8dc869f1b372');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["  primary_label                secondary_labels                     type  \\\n","0       afrsil1                              []  ['call', 'flight call']   \n","1       afrsil1  ['houspa', 'redava', 'zebdov']                 ['call']   \n","2       afrsil1                              []         ['call', 'song']   \n","3       afrsil1                              []   ['alarm call', 'call']   \n","4       afrsil1                              []          ['flight call']   \n","\n","   latitude  longitude  scientific_name         common_name          author  \\\n","0   12.3910    -1.4930  Euodice cantans  African Silverbill       Bram Piot   \n","1   19.8801  -155.7254  Euodice cantans  African Silverbill        Dan Lane   \n","2   16.2901   -16.0321  Euodice cantans  African Silverbill       Bram Piot   \n","3   17.0922    54.2958  Euodice cantans  African Silverbill  Oscar Campbell   \n","4   21.4581  -157.7252  Euodice cantans  African Silverbill   Ross Gallardy   \n","\n","                                             license  rating   time  \\\n","0  Creative Commons Attribution-NonCommercial-Sha...     2.5  08:00   \n","1  Creative Commons Attribution-NonCommercial-Sha...     3.5  08:30   \n","2  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:30   \n","3  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:00   \n","4  Creative Commons Attribution-NonCommercial-Sha...     3.0  16:30   \n","\n","                                 url              filename  \\\n","0  https://www.xeno-canto.org/125458  afrsil1/XC125458.ogg   \n","1  https://www.xeno-canto.org/175522  afrsil1/XC175522.ogg   \n","2  https://www.xeno-canto.org/177993  afrsil1/XC177993.ogg   \n","3  https://www.xeno-canto.org/205893  afrsil1/XC205893.ogg   \n","4  https://www.xeno-canto.org/207431  afrsil1/XC207431.ogg   \n","\n","                     new_target  len_new_target  \n","0                      afrsil1                1  \n","1  afrsil1 houspa redava zebdov               4  \n","2                      afrsil1                1  \n","3                      afrsil1                1  \n","4                      afrsil1                1  "]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["import ast\n","\n","\n","train = pd.read_csv('/content/drive/MyDrive/birdclef2022/birdclef-2022/train_metadata.csv')\n","\n","\n","train['new_target'] = train['primary_label'] + ' ' + train['secondary_labels'].map(lambda x: ' '.join(ast.literal_eval(x)))\n","train['len_new_target'] = train['new_target'].map(lambda x: len(x.split()))\n","# train['len_new_target'].value_counts()\n","train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6kXMqTxO0W9X","outputId":"e46892e6-efa9-44ec-e04b-af2f06180164"},"outputs":[{"data":{"text/html":["\n","  \u003cdiv id=\"df-d542d222-cd8e-42bc-8d2f-da6db2e4661f\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003efile_path\u003c/th\u003e\n","      \u003cth\u003efilename\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/birdclef2022/birdclef20...\u003c/td\u003e\n","      \u003ctd\u003eafrsil1/XC175522.ogg\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/birdclef2022/birdclef20...\u003c/td\u003e\n","      \u003ctd\u003eafrsil1/XC125458.ogg\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/birdclef2022/birdclef20...\u003c/td\u003e\n","      \u003ctd\u003eafrsil1/XC177993.ogg\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/birdclef2022/birdclef20...\u003c/td\u003e\n","      \u003ctd\u003eafrsil1/XC205893.ogg\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/birdclef2022/birdclef20...\u003c/td\u003e\n","      \u003ctd\u003eafrsil1/XC207432.ogg\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d542d222-cd8e-42bc-8d2f-da6db2e4661f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-d542d222-cd8e-42bc-8d2f-da6db2e4661f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d542d222-cd8e-42bc-8d2f-da6db2e4661f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["                                           file_path              filename\n","0  /content/drive/MyDrive/birdclef2022/birdclef20...  afrsil1/XC175522.ogg\n","1  /content/drive/MyDrive/birdclef2022/birdclef20...  afrsil1/XC125458.ogg\n","2  /content/drive/MyDrive/birdclef2022/birdclef20...  afrsil1/XC177993.ogg\n","3  /content/drive/MyDrive/birdclef2022/birdclef20...  afrsil1/XC205893.ogg\n","4  /content/drive/MyDrive/birdclef2022/birdclef20...  afrsil1/XC207432.ogg"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["path_df = pd.DataFrame(all_path, columns=['file_path'])\n","path_df['filename'] = path_df['file_path'].map(lambda x: x.split('/')[-2]+'/'+x.split('/')[-1][:-4])\n","path_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2EwUb9j40W9Y","outputId":"005f6350-9492-4490-bee5-6ca0915d6798"},"outputs":[{"name":"stdout","output_type":"stream","text":["(14852, 16)\n"]},{"data":{"text/html":["\n","  \u003cdiv id=\"df-c6322582-639c-40fe-a747-7d7b1b809775\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eprimary_label\u003c/th\u003e\n","      \u003cth\u003esecondary_labels\u003c/th\u003e\n","      \u003cth\u003etype\u003c/th\u003e\n","      \u003cth\u003elatitude\u003c/th\u003e\n","      \u003cth\u003elongitude\u003c/th\u003e\n","      \u003cth\u003escientific_name\u003c/th\u003e\n","      \u003cth\u003ecommon_name\u003c/th\u003e\n","      \u003cth\u003eauthor\u003c/th\u003e\n","      \u003cth\u003elicense\u003c/th\u003e\n","      \u003cth\u003erating\u003c/th\u003e\n","      \u003cth\u003etime\u003c/th\u003e\n","      \u003cth\u003eurl\u003c/th\u003e\n","      \u003cth\u003efilename\u003c/th\u003e\n","      \u003cth\u003enew_target\u003c/th\u003e\n","      \u003cth\u003elen_new_target\u003c/th\u003e\n","      \u003cth\u003efile_path\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003eafrsil1\u003c/td\u003e\n","      \u003ctd\u003e[]\u003c/td\u003e\n","      \u003ctd\u003e['call', 'flight call']\u003c/td\u003e\n","      \u003ctd\u003e12.3910\u003c/td\u003e\n","      \u003ctd\u003e-1.4930\u003c/td\u003e\n","      \u003ctd\u003eEuodice cantans\u003c/td\u003e\n","      \u003ctd\u003eAfrican Silverbill\u003c/td\u003e\n","      \u003ctd\u003eBram Piot\u003c/td\u003e\n","      \u003ctd\u003eCreative Commons Attribution-NonCommercial-Sha...\u003c/td\u003e\n","      \u003ctd\u003e2.5\u003c/td\u003e\n","      \u003ctd\u003e08:00\u003c/td\u003e\n","      \u003ctd\u003ehttps://www.xeno-canto.org/125458\u003c/td\u003e\n","      \u003ctd\u003eafrsil1/XC125458.ogg\u003c/td\u003e\n","      \u003ctd\u003eafrsil1\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/birdclef2022/birdclef20...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eafrsil1\u003c/td\u003e\n","      \u003ctd\u003e['houspa', 'redava', 'zebdov']\u003c/td\u003e\n","      \u003ctd\u003e['call']\u003c/td\u003e\n","      \u003ctd\u003e19.8801\u003c/td\u003e\n","      \u003ctd\u003e-155.7254\u003c/td\u003e\n","      \u003ctd\u003eEuodice cantans\u003c/td\u003e\n","      \u003ctd\u003eAfrican Silverbill\u003c/td\u003e\n","      \u003ctd\u003eDan Lane\u003c/td\u003e\n","      \u003ctd\u003eCreative Commons Attribution-NonCommercial-Sha...\u003c/td\u003e\n","      \u003ctd\u003e3.5\u003c/td\u003e\n","      \u003ctd\u003e08:30\u003c/td\u003e\n","      \u003ctd\u003ehttps://www.xeno-canto.org/175522\u003c/td\u003e\n","      \u003ctd\u003eafrsil1/XC175522.ogg\u003c/td\u003e\n","      \u003ctd\u003eafrsil1 houspa redava zebdov\u003c/td\u003e\n","      \u003ctd\u003e4\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/birdclef2022/birdclef20...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003eafrsil1\u003c/td\u003e\n","      \u003ctd\u003e[]\u003c/td\u003e\n","      \u003ctd\u003e['call', 'song']\u003c/td\u003e\n","      \u003ctd\u003e16.2901\u003c/td\u003e\n","      \u003ctd\u003e-16.0321\u003c/td\u003e\n","      \u003ctd\u003eEuodice cantans\u003c/td\u003e\n","      \u003ctd\u003eAfrican Silverbill\u003c/td\u003e\n","      \u003ctd\u003eBram Piot\u003c/td\u003e\n","      \u003ctd\u003eCreative Commons Attribution-NonCommercial-Sha...\u003c/td\u003e\n","      \u003ctd\u003e4.0\u003c/td\u003e\n","      \u003ctd\u003e11:30\u003c/td\u003e\n","      \u003ctd\u003ehttps://www.xeno-canto.org/177993\u003c/td\u003e\n","      \u003ctd\u003eafrsil1/XC177993.ogg\u003c/td\u003e\n","      \u003ctd\u003eafrsil1\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/birdclef2022/birdclef20...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003eafrsil1\u003c/td\u003e\n","      \u003ctd\u003e[]\u003c/td\u003e\n","      \u003ctd\u003e['alarm call', 'call']\u003c/td\u003e\n","      \u003ctd\u003e17.0922\u003c/td\u003e\n","      \u003ctd\u003e54.2958\u003c/td\u003e\n","      \u003ctd\u003eEuodice cantans\u003c/td\u003e\n","      \u003ctd\u003eAfrican Silverbill\u003c/td\u003e\n","      \u003ctd\u003eOscar Campbell\u003c/td\u003e\n","      \u003ctd\u003eCreative Commons Attribution-NonCommercial-Sha...\u003c/td\u003e\n","      \u003ctd\u003e4.0\u003c/td\u003e\n","      \u003ctd\u003e11:00\u003c/td\u003e\n","      \u003ctd\u003ehttps://www.xeno-canto.org/205893\u003c/td\u003e\n","      \u003ctd\u003eafrsil1/XC205893.ogg\u003c/td\u003e\n","      \u003ctd\u003eafrsil1\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/birdclef2022/birdclef20...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003eafrsil1\u003c/td\u003e\n","      \u003ctd\u003e[]\u003c/td\u003e\n","      \u003ctd\u003e['flight call']\u003c/td\u003e\n","      \u003ctd\u003e21.4581\u003c/td\u003e\n","      \u003ctd\u003e-157.7252\u003c/td\u003e\n","      \u003ctd\u003eEuodice cantans\u003c/td\u003e\n","      \u003ctd\u003eAfrican Silverbill\u003c/td\u003e\n","      \u003ctd\u003eRoss Gallardy\u003c/td\u003e\n","      \u003ctd\u003eCreative Commons Attribution-NonCommercial-Sha...\u003c/td\u003e\n","      \u003ctd\u003e3.0\u003c/td\u003e\n","      \u003ctd\u003e16:30\u003c/td\u003e\n","      \u003ctd\u003ehttps://www.xeno-canto.org/207431\u003c/td\u003e\n","      \u003ctd\u003eafrsil1/XC207431.ogg\u003c/td\u003e\n","      \u003ctd\u003eafrsil1\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/birdclef2022/birdclef20...\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6322582-639c-40fe-a747-7d7b1b809775')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-c6322582-639c-40fe-a747-7d7b1b809775 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c6322582-639c-40fe-a747-7d7b1b809775');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["  primary_label                secondary_labels                     type  \\\n","0       afrsil1                              []  ['call', 'flight call']   \n","1       afrsil1  ['houspa', 'redava', 'zebdov']                 ['call']   \n","2       afrsil1                              []         ['call', 'song']   \n","3       afrsil1                              []   ['alarm call', 'call']   \n","4       afrsil1                              []          ['flight call']   \n","\n","   latitude  longitude  scientific_name         common_name          author  \\\n","0   12.3910    -1.4930  Euodice cantans  African Silverbill       Bram Piot   \n","1   19.8801  -155.7254  Euodice cantans  African Silverbill        Dan Lane   \n","2   16.2901   -16.0321  Euodice cantans  African Silverbill       Bram Piot   \n","3   17.0922    54.2958  Euodice cantans  African Silverbill  Oscar Campbell   \n","4   21.4581  -157.7252  Euodice cantans  African Silverbill   Ross Gallardy   \n","\n","                                             license  rating   time  \\\n","0  Creative Commons Attribution-NonCommercial-Sha...     2.5  08:00   \n","1  Creative Commons Attribution-NonCommercial-Sha...     3.5  08:30   \n","2  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:30   \n","3  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:00   \n","4  Creative Commons Attribution-NonCommercial-Sha...     3.0  16:30   \n","\n","                                 url              filename  \\\n","0  https://www.xeno-canto.org/125458  afrsil1/XC125458.ogg   \n","1  https://www.xeno-canto.org/175522  afrsil1/XC175522.ogg   \n","2  https://www.xeno-canto.org/177993  afrsil1/XC177993.ogg   \n","3  https://www.xeno-canto.org/205893  afrsil1/XC205893.ogg   \n","4  https://www.xeno-canto.org/207431  afrsil1/XC207431.ogg   \n","\n","                     new_target  len_new_target  \\\n","0                      afrsil1                1   \n","1  afrsil1 houspa redava zebdov               4   \n","2                      afrsil1                1   \n","3                      afrsil1                1   \n","4                      afrsil1                1   \n","\n","                                           file_path  \n","0  /content/drive/MyDrive/birdclef2022/birdclef20...  \n","1  /content/drive/MyDrive/birdclef2022/birdclef20...  \n","2  /content/drive/MyDrive/birdclef2022/birdclef20...  \n","3  /content/drive/MyDrive/birdclef2022/birdclef20...  \n","4  /content/drive/MyDrive/birdclef2022/birdclef20...  "]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["train = pd.merge(train, path_df, on='filename')\n","print(train.shape)\n","train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9CL51Yce0W9Z","outputId":"0a899be1-e998-4ffa-d011-261ff9a18b9d"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n","  UserWarning,\n"]}],"source":["Fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","for n, (trn_index, val_index) in enumerate(Fold.split(train, train['primary_label'])):\n","    train.loc[val_index, 'kfold'] = int(n)\n","train['kfold'] = train['kfold'].astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GXPy9p4Y0W9a"},"outputs":[],"source":["train.to_csv('train_folds.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"S_b97_j30W9a"},"outputs":[],"source":["class CFG:\n","    ######################\n","    # Globals #\n","    ######################\n","    EXP_ID = 'EX005'\n","    seed = 71\n","    epochs = 6#23\n","    cutmix_and_mixup_epochs = 18\n","    folds =  [0] #[0, 1, 2, 3, 4]\n","    N_FOLDS = 5\n","    LR = 1e-3\n","    ETA_MIN = 1e-6\n","    WEIGHT_DECAY = 1e-6\n","    train_bs = 16 # 32\n","    valid_bs = 32 # 64\n","    base_model_name = \"tf_efficientnet_b0_ns\"\n","    EARLY_STOPPING = True\n","    DEBUG = False # True\n","    EVALUATION = 'AUC'\n","    apex = True\n","\n","    pooling = \"max\"\n","    pretrained = True\n","    num_classes = 152\n","    in_channels = 3\n","    target_columns = 'afrsil1 akekee akepa1 akiapo akikik amewig aniani apapan arcter \\\n","                      barpet bcnher belkin1 bkbplo bknsti bkwpet blkfra blknod bongul \\\n","                      brant brnboo brnnod brnowl brtcur bubsan buffle bulpet burpar buwtea \\\n","                      cacgoo1 calqua cangoo canvas caster1 categr chbsan chemun chukar cintea \\\n","                      comgal1 commyn compea comsan comwax coopet crehon dunlin elepai ercfra eurwig \\\n","                      fragul gadwal gamqua glwgul gnwtea golphe grbher3 grefri gresca gryfra gwfgoo \\\n","                      hawama hawcoo hawcre hawgoo hawhaw hawpet1 hoomer houfin houspa hudgod iiwi incter1 \\\n","                      jabwar japqua kalphe kauama laugul layalb lcspet leasan leater1 lessca lesyel lobdow lotjae \\\n","                      madpet magpet1 mallar3 masboo mauala maupar merlin mitpar moudov norcar norhar2 normoc norpin \\\n","                      norsho nutman oahama omao osprey pagplo palila parjae pecsan peflov perfal pibgre pomjae puaioh \\\n","                      reccar redava redjun redpha1 refboo rempar rettro ribgul rinduc rinphe rocpig rorpar rudtur ruff \\\n","                      saffin sander semplo sheowl shtsan skylar snogoo sooshe sooter1 sopsku1 sora spodov sposan \\\n","                      towsol wantat1 warwhe1 wesmea wessan wetshe whfibi whiter whttro wiltur yebcar yefcan zebdov'.split()\n","\n","    img_size = 224 # 128\n","    main_metric = \"epoch_f1_at_03\"\n","\n","    period = 6\n","    n_mels = 224 # 128\n","    fmin = 20\n","    fmax = 16000\n","    n_fft = 2048\n","    hop_length = 512#512\n","    sample_rate = 32000\n","    melspectrogram_parameters = {\n","        \"n_mels\": 224, # 128,\n","        \"fmin\": 20,\n","        \"fmax\": 16000\n","    }\n","    \n","    \n","class AudioParams:\n","    \"\"\"\n","    Parameters used for the audio data\n","    \"\"\"\n","    sr = CFG.sample_rate\n","    duration = CFG.period\n","    # Melspectrogram\n","    n_mels = CFG.n_mels\n","    fmin = CFG.fmin\n","    fmax = CFG.fmax"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BuRY4BZ50W9b"},"outputs":[],"source":["class Compose:\n","    def __init__(self, transforms: list):\n","        self.transforms = transforms\n","\n","    def __call__(self, y: np.ndarray, sr):\n","        for trns in self.transforms:\n","            y = trns(y, sr)\n","        return y\n","\n","\n","class AudioTransform:\n","    def __init__(self, always_apply=False, p=0.5):\n","        self.always_apply = always_apply\n","        self.p = p\n","\n","    def __call__(self, y: np.ndarray, sr):\n","        if self.always_apply:\n","            return self.apply(y, sr=sr)\n","        else:\n","            if np.random.rand() \u003c self.p:\n","                return self.apply(y, sr=sr)\n","            else:\n","                return y\n","\n","    def apply(self, y: np.ndarray, **params):\n","        raise NotImplementedError\n","\n","\n","class OneOf(Compose):\n","    # https://github.com/albumentations-team/albumentations/blob/master/albumentations/core/composition.py\n","    def __init__(self, transforms, p=0.5):\n","        super().__init__(transforms)\n","        self.p = p\n","        transforms_ps = [t.p for t in transforms]\n","        s = sum(transforms_ps)\n","        self.transforms_ps = [t / s for t in transforms_ps]\n","\n","    def __call__(self, y: np.ndarray, sr):\n","        data = y\n","        if self.transforms_ps and (random.random() \u003c self.p):\n","            random_state = np.random.RandomState(random.randint(0, 2 ** 32 - 1))\n","            t = random_state.choice(self.transforms, p=self.transforms_ps)\n","            data = t(y, sr)\n","        return data\n","\n","\n","class Normalize(AudioTransform):\n","    def __init__(self, always_apply=False, p=1):\n","        super().__init__(always_apply, p)\n","\n","    def apply(self, y: np.ndarray, **params):\n","        max_vol = np.abs(y).max()\n","        y_vol = y * 1 / max_vol\n","        return np.asfortranarray(y_vol)\n","\n","\n","class NewNormalize(AudioTransform):\n","    def __init__(self, always_apply=False, p=1):\n","        super().__init__(always_apply, p)\n","\n","    def apply(self, y: np.ndarray, **params):\n","        y_mm = y - y.mean()\n","        return y_mm / y_mm.abs().max()\n","\n","\n","class NoiseInjection(AudioTransform):\n","    def __init__(self, always_apply=False, p=0.5, max_noise_level=0.5):\n","        super().__init__(always_apply, p)\n","\n","        self.noise_level = (0.0, max_noise_level)\n","\n","    def apply(self, y: np.ndarray, **params):\n","        noise_level = np.random.uniform(*self.noise_level)\n","        noise = np.random.randn(len(y))\n","        augmented = (y + noise * noise_level).astype(y.dtype)\n","        return augmented\n","\n","\n","class GaussianNoise(AudioTransform):\n","    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20):\n","        super().__init__(always_apply, p)\n","\n","        self.min_snr = min_snr\n","        self.max_snr = max_snr\n","\n","    def apply(self, y: np.ndarray, **params):\n","        snr = np.random.uniform(self.min_snr, self.max_snr)\n","        a_signal = np.sqrt(y ** 2).max()\n","        a_noise = a_signal / (10 ** (snr / 20))\n","\n","        white_noise = np.random.randn(len(y))\n","        a_white = np.sqrt(white_noise ** 2).max()\n","        augmented = (y + white_noise * 1 / a_white * a_noise).astype(y.dtype)\n","        return augmented\n","\n","\n","class PinkNoise(AudioTransform):\n","    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20):\n","        super().__init__(always_apply, p)\n","\n","        self.min_snr = min_snr\n","        self.max_snr = max_snr\n","\n","    def apply(self, y: np.ndarray, **params):\n","        snr = np.random.uniform(self.min_snr, self.max_snr)\n","        a_signal = np.sqrt(y ** 2).max()\n","        a_noise = a_signal / (10 ** (snr / 20))\n","\n","        pink_noise = cn.powerlaw_psd_gaussian(1, len(y))\n","        a_pink = np.sqrt(pink_noise ** 2).max()\n","        augmented = (y + pink_noise * 1 / a_pink * a_noise).astype(y.dtype)\n","        return augmented\n","\n","\n","class PitchShift(AudioTransform):\n","    def __init__(self, always_apply=False, p=0.5, max_range=5):\n","        super().__init__(always_apply, p)\n","        self.max_range = max_range\n","\n","    def apply(self, y: np.ndarray, sr, **params):\n","        n_steps = np.random.randint(-self.max_range, self.max_range)\n","        augmented = librosa.effects.pitch_shift(y, sr, n_steps)\n","        return augmented\n","\n","\n","class TimeStretch(AudioTransform):\n","    def __init__(self, always_apply=False, p=0.5, max_rate=1):\n","        super().__init__(always_apply, p)\n","        self.max_rate = max_rate\n","\n","    def apply(self, y: np.ndarray, **params):\n","        rate = np.random.uniform(0, self.max_rate)\n","        augmented = librosa.effects.time_stretch(y, rate)\n","        return augmented\n","\n","\n","def _db2float(db: float, amplitude=True):\n","    if amplitude:\n","        return 10 ** (db / 20)\n","    else:\n","        return 10 ** (db / 10)\n","\n","\n","def volume_down(y: np.ndarray, db: float):\n","    \"\"\"\n","    Low level API for decreasing the volume\n","    Parameters\n","    ----------\n","    y: numpy.ndarray\n","        stereo / monaural input audio\n","    db: float\n","        how much decibel to decrease\n","    Returns\n","    -------\n","    applied: numpy.ndarray\n","        audio with decreased volume\n","    \"\"\"\n","    applied = y * _db2float(-db)\n","    return applied\n","\n","\n","def volume_up(y: np.ndarray, db: float):\n","    \"\"\"\n","    Low level API for increasing the volume\n","    Parameters\n","    ----------\n","    y: numpy.ndarray\n","        stereo / monaural input audio\n","    db: float\n","        how much decibel to increase\n","    Returns\n","    -------\n","    applied: numpy.ndarray\n","        audio with increased volume\n","    \"\"\"\n","    applied = y * _db2float(db)\n","    return applied\n","\n","\n","class RandomVolume(AudioTransform):\n","    def __init__(self, always_apply=False, p=0.5, limit=10):\n","        super().__init__(always_apply, p)\n","        self.limit = limit\n","\n","    def apply(self, y: np.ndarray, **params):\n","        db = np.random.uniform(-self.limit, self.limit)\n","        if db \u003e= 0:\n","            return volume_up(y, db)\n","        else:\n","            return volume_down(y, db)\n","\n","\n","class CosineVolume(AudioTransform):\n","    def __init__(self, always_apply=False, p=0.5, limit=10):\n","        super().__init__(always_apply, p)\n","        self.limit = limit\n","\n","    def apply(self, y: np.ndarray, **params):\n","        db = np.random.uniform(-self.limit, self.limit)\n","        cosine = np.cos(np.arange(len(y)) / len(y) * np.pi * 2)\n","        dbs = _db2float(cosine * db)\n","        return y * dbs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6c-QVZfx0W9e"},"outputs":[],"source":["import os\n","\n","OUTPUT_DIR = f'./'\n","if not os.path.exists(OUTPUT_DIR):\n","    os.makedirs(OUTPUT_DIR)\n","   \n","    \n","def set_seed(seed=42):\n","    random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    \n","set_seed(CFG.seed)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Qkoech4z0W9e"},"outputs":[],"source":["def calc_loss(y_true, y_pred):\n","    return metrics.roc_auc_score(np.array(y_true), np.array(y_pred))\n","\n","\n","# ====================================================\n","# Training helper functions\n","# ====================================================\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        \n","\n","class MetricMeter(object):\n","    def __init__(self):\n","        self.reset()\n","    \n","    def reset(self):\n","        self.y_true = []\n","        self.y_pred = []\n","    \n","    def update(self, y_true, y_pred):\n","        self.y_true.extend(y_true.cpu().detach().numpy().tolist())\n","        self.y_pred.extend(y_pred[\"clipwise_output\"].cpu().detach().numpy().tolist())\n","\n","    @property\n","    def avg(self):\n","        self.f1_03 = metrics.f1_score(np.array(self.y_true), np.array(self.y_pred) \u003e 0.3, average=\"micro\")\n","        self.f1_05 = metrics.f1_score(np.array(self.y_true), np.array(self.y_pred) \u003e 0.5, average=\"micro\")\n","        \n","        return {\n","            \"f1_at_03\" : self.f1_03,\n","            \"f1_at_05\" : self.f1_05,\n","        }\n","    \n","    \n","# https://www.kaggle.com/c/rfcx-species-audio-detection/discussion/213075\n","class BCEFocalLoss(nn.Module):\n","    def __init__(self, alpha=0.25, gamma=2.0):\n","        super().__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, preds, targets):\n","        bce_loss = nn.BCEWithLogitsLoss(reduction='none')(preds, targets)\n","        probas = torch.sigmoid(preds)\n","        loss = targets * self.alpha * \\\n","            (1. - probas)**self.gamma * bce_loss + \\\n","            (1. - targets) * probas**self.gamma * bce_loss\n","        loss = loss.mean()\n","        return loss\n","\n","\n","class BCEFocal2WayLoss(nn.Module):\n","    def __init__(self, weights=[1, 1], class_weights=None):\n","        super().__init__()\n","\n","        self.focal = BCEFocalLoss()\n","\n","        self.weights = weights\n","\n","    def forward(self, input, target):\n","        input_ = input[\"logit\"]\n","        target = target.float()\n","\n","        framewise_output = input[\"framewise_logit\"]\n","        clipwise_output_with_max, _ = framewise_output.max(dim=1)\n","\n","        loss = self.focal(input_, target)\n","        aux_loss = self.focal(clipwise_output_with_max, target)\n","\n","        return self.weights[0] * loss + self.weights[1] * aux_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"V8qXFHLV0W9f","outputId":"246abd19-f544-40c3-8c6e-d8865df99bd8"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/dropout/cutout.py:52: FutureWarning: Cutout has been deprecated. Please use CoarseDropout\n","  FutureWarning,\n"]}],"source":["def compute_melspec(y, params):\n","    \"\"\"\n","    Computes a mel-spectrogram and puts it at decibel scale\n","    Arguments:\n","        y {np array} -- signal\n","        params {AudioParams} -- Parameters to use for the spectrogram. Expected to have the attributes sr, n_mels, f_min, f_max\n","    Returns:\n","        np array -- Mel-spectrogram\n","    \"\"\"\n","    melspec = librosa.feature.melspectrogram(\n","        y=y, sr=params.sr, n_mels=params.n_mels, fmin=params.fmin, fmax=params.fmax,\n","    )\n","\n","    melspec = librosa.power_to_db(melspec).astype(np.float32)\n","    return melspec\n","\n","\n","def crop_or_pad(y, length, sr, train=True, probs=None):\n","    \"\"\"\n","    Crops an array to a chosen length\n","    Arguments:\n","        y {1D np array} -- Array to crop\n","        length {int} -- Length of the crop\n","        sr {int} -- Sampling rate\n","    Keyword Arguments:\n","        train {bool} -- Whether we are at train time. If so, crop randomly, else return the beginning of y (default: {True})\n","        probs {None or numpy array} -- Probabilities to use to chose where to crop (default: {None})\n","    Returns:\n","        1D np array -- Cropped array\n","    \"\"\"\n","    if len(y) \u003c= length:\n","        y = np.concatenate([y, np.zeros(length - len(y))])\n","    else:\n","        if not train:\n","            start = 0\n","        elif probs is None:\n","            start = np.random.randint(len(y) - length)\n","        else:\n","            start = (\n","                    np.random.choice(np.arange(len(probs)), p=probs) + np.random.random()\n","            )\n","            start = int(sr * (start))\n","\n","        y = y[start: start + length]\n","\n","    return y.astype(np.float32)\n","\n","\n","def mono_to_color(X, eps=1e-6, mean=None, std=None):\n","    \"\"\"\n","    Converts a one channel array to a 3 channel one in [0, 255]\n","    Arguments:\n","        X {numpy array [H x W]} -- 2D array to convert\n","    Keyword Arguments:\n","        eps {float} -- To avoid dividing by 0 (default: {1e-6})\n","        mean {None or np array} -- Mean for normalization (default: {None})\n","        std {None or np array} -- Std for normalization (default: {None})\n","    Returns:\n","        numpy array [3 x H x W] -- RGB numpy array\n","    \"\"\"\n","    X = np.stack([X, X, X], axis=-1)\n","\n","    # Standardize\n","    mean = mean or X.mean()\n","    std = std or X.std()\n","    X = (X - mean) / (std + eps)\n","\n","    # Normalize to [0, 255]\n","    _min, _max = X.min(), X.max()\n","\n","    if (_max - _min) \u003e eps:\n","        V = np.clip(X, _min, _max)\n","        V = 255 * (V - _min) / (_max - _min)\n","        V = V.astype(np.uint8)\n","    else:\n","        V = np.zeros_like(X, dtype=np.uint8)\n","\n","    return V\n","\n","\n","mean = (0.485, 0.456, 0.406) # RGB\n","std = (0.229, 0.224, 0.225) # RGB\n","\n","albu_transforms = {\n","    'train' : A.Compose([\n","            A.HorizontalFlip(p=0.5),\n","            A.OneOf([\n","                A.Cutout(max_h_size=5, max_w_size=16),\n","                A.CoarseDropout(max_holes=4),\n","            ], p=0.5),\n","            A.Normalize(mean, std),\n","    ]),\n","    'valid' : A.Compose([\n","            A.Normalize(mean, std),\n","    ]),\n","}\n","\n","\n","class WaveformDataset(torch.utils.data.Dataset):\n","    def __init__(self,\n","                 df: pd.DataFrame,\n","                 mode='train'):\n","        self.df = df\n","        self.mode = mode\n","\n","        if mode == 'train':\n","            self.wave_transforms = Compose(\n","                [\n","                    OneOf(\n","                        [\n","                            NoiseInjection(p=1, max_noise_level=0.04),\n","                            GaussianNoise(p=1, min_snr=5, max_snr=20),\n","                            PinkNoise(p=1, min_snr=5, max_snr=20),\n","                        ],\n","                        p=0.2,\n","                    ),\n","                    RandomVolume(p=0.2, limit=4),\n","                    Normalize(p=1),\n","                ]\n","            )\n","        else:\n","            self.wave_transforms = Compose(\n","                [\n","                    Normalize(p=1),\n","                ]\n","            )\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx: int):\n","        SR = 32000\n","        sample = self.df.loc[idx, :]\n","        \n","        wav_path = sample[\"file_path\"]\n","        labels = sample[\"new_target\"]\n","\n","        y = np.load(wav_path)\n","\n","        # SEC = int(len(y)/2/SR)\n","        # if SEC \u003e 0:\n","        #     start = np.random.randint(SEC)\n","        #     end = start+AudioParams.duration\n","        if len(y) \u003e 0:\n","            y = y[:AudioParams.duration*SR]\n","\n","            if self.wave_transforms:\n","                y = self.wave_transforms(y, sr=SR)\n","\n","        y = np.concatenate([y, y, y])[:AudioParams.duration * AudioParams.sr] \n","        y = crop_or_pad(y, AudioParams.duration * AudioParams.sr, sr=AudioParams.sr, train=True, probs=None)\n","        image = compute_melspec(y, AudioParams)\n","        image = mono_to_color(image)\n","        image = image.astype(np.uint8)\n","        \n","        # image = np.load(wav_path) # (224, 313, 3)\n","        image = albu_transforms[self.mode](image=image)['image']\n","        image = image.T\n","        \n","        targets = np.zeros(len(CFG.target_columns), dtype=float)\n","        for ebird_code in labels.split():\n","            targets[CFG.target_columns.index(ebird_code)] = 1.0\n","\n","        return {\n","            \"image\": image,\n","            \"targets\": targets,\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kkmr6FBI0W9h"},"outputs":[],"source":["def init_layer(layer):\n","    nn.init.xavier_uniform_(layer.weight)\n","\n","    if hasattr(layer, \"bias\"):\n","        if layer.bias is not None:\n","            layer.bias.data.fill_(0.)\n","\n","\n","def init_bn(bn):\n","    bn.bias.data.fill_(0.)\n","    bn.weight.data.fill_(1.0)\n","\n","\n","def init_weights(model):\n","    classname = model.__class__.__name__\n","    if classname.find(\"Conv2d\") != -1:\n","        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n","        model.bias.data.fill_(0)\n","    elif classname.find(\"BatchNorm\") != -1:\n","        model.weight.data.normal_(1.0, 0.02)\n","        model.bias.data.fill_(0)\n","    elif classname.find(\"GRU\") != -1:\n","        for weight in model.parameters():\n","            if len(weight.size()) \u003e 1:\n","                nn.init.orghogonal_(weight.data)\n","    elif classname.find(\"Linear\") != -1:\n","        model.weight.data.normal_(0, 0.01)\n","        model.bias.data.zero_()\n","\n","\n","def interpolate(x: torch.Tensor, ratio: int):\n","    \"\"\"Interpolate data in time domain. This is used to compensate the\n","    resolution reduction in downsampling of a CNN.\n","    Args:\n","      x: (batch_size, time_steps, classes_num)\n","      ratio: int, ratio to interpolate\n","    Returns:\n","      upsampled: (batch_size, time_steps * ratio, classes_num)\n","    \"\"\"\n","    (batch_size, time_steps, classes_num) = x.shape\n","    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n","    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n","    return upsampled\n","\n","\n","def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n","    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n","    is the same as the value of the last frame.\n","    Args:\n","      framewise_output: (batch_size, frames_num, classes_num)\n","      frames_num: int, number of frames to pad\n","    Outputs:\n","      output: (batch_size, frames_num, classes_num)\n","    \"\"\"\n","    output = F.interpolate(\n","        framewise_output.unsqueeze(1),\n","        size=(frames_num, framewise_output.size(2)),\n","        align_corners=True,\n","        mode=\"bilinear\").squeeze(1)\n","\n","    return output\n","\n","\n","class AttBlockV2(nn.Module):\n","    def __init__(self,\n","                 in_features: int,\n","                 out_features: int,\n","                 activation=\"linear\"):\n","        super().__init__()\n","\n","        self.activation = activation\n","        self.att = nn.Conv1d(\n","            in_channels=in_features,\n","            out_channels=out_features,\n","            kernel_size=1,\n","            stride=1,\n","            padding=0,\n","            bias=True)\n","        self.cla = nn.Conv1d(\n","            in_channels=in_features,\n","            out_channels=out_features,\n","            kernel_size=1,\n","            stride=1,\n","            padding=0,\n","            bias=True)\n","\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        init_layer(self.att)\n","        init_layer(self.cla)\n","\n","    def forward(self, x):\n","        # x: (n_samples, n_in, n_time)\n","        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n","        cla = self.nonlinear_transform(self.cla(x))\n","        x = torch.sum(norm_att * cla, dim=2)\n","        return x, norm_att, cla\n","\n","    def nonlinear_transform(self, x):\n","        if self.activation == 'linear':\n","            return x\n","        elif self.activation == 'sigmoid':\n","            return torch.sigmoid(x)\n","\n","\n","class TimmSED(nn.Module):\n","    def __init__(self, base_model_name: str, pretrained=False, num_classes=24, in_channels=1):\n","        super().__init__()\n","\n","        self.spec_augmenter = SpecAugmentation(time_drop_width=64//2, time_stripes_num=2,\n","                                               freq_drop_width=8//2, freq_stripes_num=2)\n","\n","        self.bn0 = nn.BatchNorm2d(CFG.n_mels)\n","\n","        base_model = timm.create_model(\n","            base_model_name, pretrained=pretrained, in_chans=in_channels)\n","        layers = list(base_model.children())[:-2]\n","        self.encoder = nn.Sequential(*layers)\n","\n","        if hasattr(base_model, \"fc\"):\n","            in_features = base_model.fc.in_features\n","        else:\n","            in_features = base_model.classifier.in_features\n","\n","        self.fc1 = nn.Linear(in_features, in_features, bias=True)\n","        self.att_block = AttBlockV2(\n","            in_features, num_classes, activation=\"sigmoid\")\n","\n","        self.init_weight()\n","\n","    def init_weight(self):\n","        init_bn(self.bn0)\n","        init_layer(self.fc1)\n","        \n","\n","    def forward(self, input_data):\n","        x = input_data # (batch_size, 3, time_steps, mel_bins)\n","\n","        frames_num = x.shape[2]\n","\n","        x = x.transpose(1, 3)\n","        x = self.bn0(x)\n","        x = x.transpose(1, 3)\n","\n","        if self.training:\n","            if random.random() \u003c 0.25:\n","                x = self.spec_augmenter(x)\n","\n","        x = x.transpose(2, 3)\n","\n","        x = self.encoder(x)\n","        \n","        # Aggregate in frequency axis\n","        x = torch.mean(x, dim=3)\n","        # x = torch.mean(x, dim=2)\n","\n","        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n","        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n","        x = x1 + x2\n","\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = x.transpose(1, 2)\n","        x = F.relu_(self.fc1(x))\n","        x = x.transpose(1, 2)\n","        x = F.dropout(x, p=0.5, training=self.training)\n","\n","        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n","        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n","        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n","        segmentwise_output = segmentwise_output.transpose(1, 2)\n","\n","        interpolate_ratio = frames_num // segmentwise_output.size(1)\n","\n","        # Get framewise output\n","        framewise_output = interpolate(segmentwise_output,\n","                                       interpolate_ratio)\n","        framewise_output = pad_framewise_output(framewise_output, frames_num)\n","\n","        framewise_logit = interpolate(segmentwise_logit, interpolate_ratio)\n","        framewise_logit = pad_framewise_output(framewise_logit, frames_num)\n","\n","        output_dict = {\n","            'framewise_output': framewise_output,\n","            'clipwise_output': clipwise_output,\n","            'logit': logit,\n","            'framewise_logit': framewise_logit,\n","        }\n","\n","        return output_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"clFuCxJb0W9h"},"outputs":[],"source":["def rand_bbox(size, lam):\n","    W = size[2]\n","    H = size[3]\n","    cut_rat = np.sqrt(1. - lam)\n","    cut_w = int(W * cut_rat)\n","    cut_h = int(H * cut_rat)\n","\n","    # uniform\n","    cx = np.random.randint(W)\n","    cy = np.random.randint(H)\n","\n","    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n","    bby1 = np.clip(cy - cut_h // 2, 0, H)\n","    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n","    bby2 = np.clip(cy + cut_h // 2, 0, H)\n","\n","    return bbx1, bby1, bbx2, bby2\n","\n","\n","def cutmix(data, targets, alpha):\n","    indices = torch.randperm(data.size(0))\n","    shuffled_data = data[indices]\n","    shuffled_targets = targets[indices]\n","\n","    lam = np.random.beta(alpha, alpha)\n","    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n","    data[:, :, bbx1:bbx2, bby1:bby2] = data[indices, :, bbx1:bbx2, bby1:bby2]\n","    # adjust lambda to exactly match pixel ratio\n","    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n","\n","    new_targets = [targets, shuffled_targets, lam]\n","    return data, new_targets\n","\n","def mixup(data, targets, alpha):\n","    indices = torch.randperm(data.size(0))\n","    shuffled_data = data[indices]\n","    shuffled_targets = targets[indices]\n","\n","    lam = np.random.beta(alpha, alpha)\n","    new_data = data * lam + shuffled_data * (1 - lam)\n","    new_targets = [targets, shuffled_targets, lam]\n","    return new_data, new_targets\n","\n","\n","def cutmix_criterion(preds, new_targets):\n","    targets1, targets2, lam = new_targets[0], new_targets[1], new_targets[2]\n","    criterion = BCEFocal2WayLoss()\n","    return lam * criterion(preds, targets1) + (1 - lam) * criterion(preds, targets2)\n","\n","def mixup_criterion(preds, new_targets):\n","    targets1, targets2, lam = new_targets[0], new_targets[1], new_targets[2]\n","    criterion = BCEFocal2WayLoss()\n","    return lam * criterion(preds, targets1) + (1 - lam) * criterion(preds, targets2)\n","\n","\n","def loss_fn(logits, targets):\n","    loss_fct = BCEFocal2WayLoss()\n","    loss = loss_fct(logits, targets)\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"DjxZvoA30W9i"},"outputs":[],"source":["def train_fn(model, data_loader, device, optimizer, scheduler):\n","    model.train()\n","    scaler = GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    scores = MetricMeter()\n","    tk0 = tqdm(data_loader, total=len(data_loader))\n","    \n","    for data in tk0:\n","        optimizer.zero_grad()\n","        inputs = data['image'].to(device)\n","        targets = data['targets'].to(device)\n","        with autocast(enabled=CFG.apex):\n","            outputs = model(inputs)\n","            loss = loss_fn(outputs, targets)\n","        \n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        \n","        scheduler.step()\n","        losses.update(loss.item(), inputs.size(0))\n","        scores.update(targets, outputs)\n","        tk0.set_postfix(loss=losses.avg)\n","    return scores.avg, losses.avg\n","\n","\n","def train_mixup_cutmix_fn(model, data_loader, device, optimizer, scheduler):\n","    model.train()\n","    scaler = GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    scores = MetricMeter()\n","    tk0 = tqdm(data_loader, total=len(data_loader))\n","\n","    for data in tk0:\n","        optimizer.zero_grad()\n","        inputs = data['image'].to(device)\n","        targets = data['targets'].to(device)\n","\n","        if np.random.rand()\u003c0.5:\n","            inputs, new_targets = mixup(inputs, targets, 0.4)\n","            with autocast(enabled=CFG.apex):\n","                outputs = model(inputs)\n","                loss = mixup_criterion(outputs, new_targets) \n","        else:\n","            inputs, new_targets = cutmix(inputs, targets, 0.4)\n","            with autocast(enabled=CFG.apex):\n","                outputs = model(inputs)\n","                loss = cutmix_criterion(outputs, new_targets)\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        \n","        scheduler.step()\n","        losses.update(loss.item(), inputs.size(0))\n","        scores.update(new_targets[0], outputs)\n","        tk0.set_postfix(loss=losses.avg)\n","    return scores.avg, losses.avg\n","\n","\n","def valid_fn(model, data_loader, device):\n","    model.eval()\n","    losses = AverageMeter()\n","    scores = MetricMeter()\n","    tk0 = tqdm(data_loader, total=len(data_loader))\n","    valid_preds = []\n","    with torch.no_grad():\n","        for data in tk0:\n","            inputs = data['image'].to(device)\n","            targets = data['targets'].to(device)\n","            outputs = model(inputs)\n","            loss = loss_fn(outputs, targets)\n","            losses.update(loss.item(), inputs.size(0))\n","            scores.update(targets, outputs)\n","            tk0.set_postfix(loss=losses.avg)\n","    return scores.avg, losses.avg"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"umayR2Ho0W9k"},"outputs":[],"source":["def inference_fn(model, data_loader, device):\n","    model.eval()\n","    tk0 = tqdm(data_loader, total=len(data_loader))\n","    final_output = []\n","    final_target = []\n","    with torch.no_grad():\n","        for b_idx, data in enumerate(tk0):\n","            inputs = data['image'].to(device)\n","            targets = data['targets'].to(device).detach().cpu().numpy().tolist()\n","            output = model(inputs)\n","            output = output[\"clipwise_output\"].cpu().detach().cpu().numpy().tolist()\n","            final_output.extend(output)\n","            final_target.extend(targets)\n","    return final_output, final_target\n","\n","\n","def calc_cv(model_paths):\n","    df = pd.read_csv('train_folds.csv')\n","    y_true = []\n","    y_pred = []\n","    for fold, model_path in enumerate(model_paths):\n","        model = TimmSED(\n","            base_model_name=CFG.base_model_name,\n","            pretrained=CFG.pretrained,\n","            num_classes=CFG.num_classes,\n","            in_channels=CFG.in_channels)\n","\n","        model.to(device)\n","        model.load_state_dict(torch.load(model_path))\n","        model.eval()\n","\n","        val_df = df[df.kfold == fold].reset_index(drop=True)\n","        dataset = WaveformDataset(df=val_df, mode='valid')\n","        dataloader = torch.utils.data.DataLoader(\n","            dataset, batch_size=CFG.valid_bs, num_workers=0, pin_memory=True, shuffle=False\n","        )\n","\n","        final_output, final_target = inference_fn(model, dataloader, device)\n","        y_pred.extend(final_output)\n","        y_true.extend(final_target)\n","        torch.cuda.empty_cache()\n","\n","        f1_03 = metrics.f1_score(np.array(y_true), np.array(y_pred) \u003e 0.3, average=\"micro\")\n","        print(f'micro f1_0.3 {f1_03}')\n","\n","    f1_03 = metrics.f1_score(np.array(y_true), np.array(y_pred) \u003e 0.3, average=\"micro\")\n","    f1_05 = metrics.f1_score(np.array(y_true), np.array(y_pred) \u003e 0.5, average=\"micro\")\n","\n","    print(f'overall micro f1_0.3 {f1_03}')\n","    print(f'overall micro f1_0.5 {f1_05}')\n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"_W0OjxSt0W9l"},"outputs":[{"name":"stdout","output_type":"stream","text":["====================================================================================================\n","Fold 0 Training\n","====================================================================================================\n"]},{"name":"stderr","output_type":"stream","text":["Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_ns-c0e6a31c.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientnet_b0_ns-c0e6a31c.pth\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n","  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"]},{"name":"stdout","output_type":"stream","text":["Starting 1 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/743 [00:00\u003c?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n","100%|██████████| 743/743 [1:19:23\u003c00:00,  6.41s/it, loss=0.0143]\n","100%|██████████| 93/93 [15:15\u003c00:00,  9.85s/it, loss=0.00797]\n","/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n","  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.01434  avg_val_loss: 0.00797  time: 5681s\n","Epoch 1 - train_f1_at_03:0.00456  valid_f1_at_03:0.00302\n","Epoch 1 - train_f1_at_05:0.00135  valid_f1_at_05:0.00000\n","\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e Model Improved From -inf ----\u003e 0.0030175015087507543\n","other scores here... 0.0030175015087507543, 0.0\n","Starting 2 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/743 [00:00\u003c?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n","100%|██████████| 743/743 [1:01:14\u003c00:00,  4.95s/it, loss=0.00837]\n","100%|██████████| 93/93 [12:58\u003c00:00,  8.37s/it, loss=0.00653]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.00837  avg_val_loss: 0.00653  time: 4455s\n","Epoch 2 - train_f1_at_03:0.01462  valid_f1_at_03:0.21989\n","Epoch 2 - train_f1_at_05:0.00030  valid_f1_at_05:0.02977\n","\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e Model Improved From 0.0030175015087507543 ----\u003e 0.21988723731419788\n","other scores here... 0.21988723731419788, 0.029770765108663295\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n","  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"]},{"name":"stdout","output_type":"stream","text":["Starting 3 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/743 [00:00\u003c?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n","100%|██████████| 743/743 [55:26\u003c00:00,  4.48s/it, loss=0.00761]\n","100%|██████████| 93/93 [10:50\u003c00:00,  6.99s/it, loss=0.00593]\n","/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n","  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.00761  avg_val_loss: 0.00593  time: 3978s\n","Epoch 3 - train_f1_at_03:0.06545  valid_f1_at_03:0.38383\n","Epoch 3 - train_f1_at_05:0.00538  valid_f1_at_05:0.19107\n","\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e Model Improved From 0.21988723731419788 ----\u003e 0.3838299645759533\n","other scores here... 0.3838299645759533, 0.19107240065323897\n","Starting 4 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/743 [00:00\u003c?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n","100%|██████████| 743/743 [1:03:35\u003c00:00,  5.14s/it, loss=0.00696]\n","100%|██████████| 93/93 [13:57\u003c00:00,  9.00s/it, loss=0.00554]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.00696  avg_val_loss: 0.00554  time: 4654s\n","Epoch 4 - train_f1_at_03:0.12228  valid_f1_at_03:0.42515\n","Epoch 4 - train_f1_at_05:0.01378  valid_f1_at_05:0.21245\n","\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e Model Improved From 0.3838299645759533 ----\u003e 0.4251497005988024\n","other scores here... 0.4251497005988024, 0.21244635193133046\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n","  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"]},{"name":"stdout","output_type":"stream","text":["Starting 5 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/743 [00:00\u003c?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n","100%|██████████| 743/743 [1:10:17\u003c00:00,  5.68s/it, loss=0.00658]\n","100%|██████████| 93/93 [14:55\u003c00:00,  9.63s/it, loss=0.00483]\n","/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n","  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.00658  avg_val_loss: 0.00483  time: 5115s\n","Epoch 5 - train_f1_at_03:0.18182  valid_f1_at_03:0.50128\n","Epoch 5 - train_f1_at_05:0.02845  valid_f1_at_05:0.28293\n","\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e Model Improved From 0.4251497005988024 ----\u003e 0.5012817984618417\n","other scores here... 0.5012817984618417, 0.2829268292682927\n","Starting 6 epoch...\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/743 [00:00\u003c?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n","100%|██████████| 743/743 [1:07:07\u003c00:00,  5.42s/it, loss=0.00648]\n","100%|██████████| 93/93 [14:25\u003c00:00,  9.31s/it, loss=0.00438]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6 - avg_train_loss: 0.00648  avg_val_loss: 0.00438  time: 4895s\n","Epoch 6 - train_f1_at_03:0.20246  valid_f1_at_03:0.56088\n","Epoch 6 - train_f1_at_05:0.03178  valid_f1_at_05:0.33667\n","\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e Model Improved From 0.5012817984618417 ----\u003e 0.5608760207869339\n","other scores here... 0.5608760207869339, 0.3366683341670836\n"]}],"source":["# main loop\n","import pickle\n","\n","for fold in range(5):\n","    if fold not in CFG.folds:\n","        continue\n","    print(\"=\" * 100)\n","    print(f\"Fold {fold} Training\")\n","    print(\"=\" * 100)\n","\n","    trn_df = train[train.kfold != fold].reset_index(drop=True)\n","    val_df = train[train.kfold == fold].reset_index(drop=True)\n","\n","    train_dataset = WaveformDataset(df=trn_df, mode='train')\n","    train_dataloader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=CFG.train_bs, num_workers=0, pin_memory=True, shuffle=True\n","    )\n","    \n","    valid_dataset = WaveformDataset(df=val_df, mode='valid')\n","    valid_dataloader = torch.utils.data.DataLoader(\n","        valid_dataset, batch_size=CFG.valid_bs, num_workers=0, pin_memory=True, shuffle=False\n","    )\n","\n","    model = TimmSED(\n","        base_model_name=CFG.base_model_name,\n","        pretrained=CFG.pretrained,\n","        num_classes=CFG.num_classes,\n","        in_channels=CFG.in_channels)\n","\n","    optimizer = transformers.AdamW(model.parameters(), lr=CFG.LR, weight_decay=CFG.WEIGHT_DECAY)\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=CFG.ETA_MIN, T_max=500)\n","\n","    model = model.to(device)\n","\n","    min_loss = 999\n","    best_score = -np.inf\n","\n","    for epoch in range(CFG.epochs):\n","        print(\"Starting {} epoch...\".format(epoch+1))\n","\n","        start_time = time.time()\n","\n","        if epoch \u003c CFG.cutmix_and_mixup_epochs:\n","            train_avg, train_loss = train_mixup_cutmix_fn(model, train_dataloader, device, optimizer, scheduler)\n","        else: \n","            train_avg, train_loss = train_fn(model, train_dataloader, device, optimizer, scheduler)\n","\n","        valid_avg, valid_loss = valid_fn(model, valid_dataloader, device)\n","\n","        elapsed = time.time() - start_time\n","\n","        print(f'Epoch {epoch+1} - avg_train_loss: {train_loss:.5f}  avg_val_loss: {valid_loss:.5f}  time: {elapsed:.0f}s')\n","        print(f\"Epoch {epoch+1} - train_f1_at_03:{train_avg['f1_at_03']:0.5f}  valid_f1_at_03:{valid_avg['f1_at_03']:0.5f}\")\n","        print(f\"Epoch {epoch+1} - train_f1_at_05:{train_avg['f1_at_05']:0.5f}  valid_f1_at_05:{valid_avg['f1_at_05']:0.5f}\")\n","\n","        if valid_avg['f1_at_03'] \u003e best_score:\n","            print(f\"\u003e\u003e\u003e\u003e\u003e\u003e\u003e\u003e Model Improved From {best_score} ----\u003e {valid_avg['f1_at_03']}\")\n","            print(f\"other scores here... {valid_avg['f1_at_03']}, {valid_avg['f1_at_05']}\")\n","            torch.save(model.state_dict(), f'fold-{fold}.bin')\n","#             pickle.dump(model, open(f'fold-{fold}_new.bin', 'wb'))\n","            best_score = valid_avg['f1_at_03']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vU3iN50j0W9l"},"outputs":[],"source":["model_paths = [f'fold-{i}.bin' for i in CFG.folds]\n","\n","calc_cv(model_paths)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nZd7nNFg0W9l"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"birdclef2022_use_2nd_label_f0_(3)_colab.ipynb","version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":0}
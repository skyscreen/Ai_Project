{"cells":[{"cell_type":"markdown","metadata":{"id":"V8TX5ncPVNY3"},"source":["# Comments:\n","    \n","This is an improvement of my baseline, you can find it here: https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7963\n","\n","The main difference between this solution and previous one is that we add new features and do seed blend to boost LB. Single 5 kfold model using seed 42 achieve an out of folds CV of 0.7977 and a public leaderboard of 0.799. If we use seed blend (train three different models using seed 42, 52, 62 and then average predictions) the LB boost niceley.\n","\n","The main features that boost CV are the following:\n","\n","* The difference between last value and the lag1\n","* The difference between last value and the average (this features gives a nice boost)\n","\n","This feature engineer is done on all the last columns, so we actually add a lot of features, this model used 1368 features.\n","\n","I uploaded test predictions to avoid running training and inference\n","\n","Next Steps:\n","\n","* Could try feature selection, maybe a lot of the feature are just noise, actually I perform permutation importance and I reduce the amount of features to 1000 app and the CV was almost the same. Maybe there is a better feature selection technique that can boost performance.\n","\n","* Could try different models, maybe some neural network with the same features or a subset of the features and then blend with LGBM can work, in my experience blending tree models and neural network works great because they are very diverse so the boost is nice\n","\n","* Could try more feature engineering, maybe we can create more features that extract the hidden signal of the dataset, actually I would first work on this option and really try to capture all the signal that the dataset has."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U38gJrkJVPTd"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"oem3LtDHVNY_"},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQ1JQbKOVNZA"},"outputs":[],"source":["# ====================================================\n","# Configurations\n","# ====================================================\n","class CFG:\n","    input_dir = '/content/drive/MyDrive/amex/output/7977o/'\n","    seed = 103\n","    n_folds = 5\n","    target = 'target'\n","    boosting_type = 'dart'\n","    metric = 'binary_logloss'\n","# ====================================================\n","# Library\n","# ====================================================\n","import gc\n","import warnings\n","warnings.filterwarnings('ignore')\n","import scipy as sp\n","import numpy as np\n","import pandas as pd\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","from tqdm.auto import tqdm\n","import itertools\n","\n","# ====================================================\n","# Get the difference\n","# ====================================================\n","def get_difference(data, num_features):\n","    df1 = []\n","    customer_ids = []\n","    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n","        # Get the differences\n","        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n","        # Append to lists\n","        df1.append(diff_df1)\n","        customer_ids.append(customer_id)\n","    # Concatenate\n","    df1 = np.concatenate(df1, axis = 0)\n","    # Transform to dataframe\n","    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n","    # Add customer id\n","    df1['customer_ID'] = customer_ids\n","    return df1\n","\n","# ====================================================\n","# Read & preprocess data and save it to disk\n","# ====================================================\n","def read_preprocess_data():\n","    train = pd.read_parquet('/content/drive/MyDrive/amex/data/amex-data-integer-dtypes-parquet-format/train.parquet')\n","    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n","    cat_features = [\n","        \"B_30\",\n","        \"B_38\",\n","        \"D_114\",\n","        \"D_116\",\n","        \"D_117\",\n","        \"D_120\",\n","        \"D_126\",\n","        \"D_63\",\n","        \"D_64\",\n","        \"D_66\",\n","        \"D_68\",\n","    ]\n","    num_features = [col for col in features if col not in cat_features]\n","    print('Starting training feature engineer...')\n","    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n","    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n","    train_num_agg.reset_index(inplace = True)\n","    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n","    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n","    train_cat_agg.reset_index(inplace = True)\n","    train_labels = pd.read_csv('/content/drive/MyDrive/amex/data/train_labels.csv')\n","    # Transform float64 columns to float32\n","    cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n","    for col in tqdm(cols):\n","        train_num_agg[col] = train_num_agg[col].astype(np.float32)\n","    # Transform int64 columns to int32\n","    cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n","    for col in tqdm(cols):\n","        train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n","    # Get the difference\n","    train_diff = get_difference(train, num_features)\n","    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n","    del train_num_agg, train_cat_agg, train_diff\n","    gc.collect()\n","    test = pd.read_parquet('/content/drive/MyDrive/amex/data/amex-data-integer-dtypes-parquet-format/test.parquet')\n","    print('Starting test feature engineer...')\n","    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n","    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n","    test_num_agg.reset_index(inplace = True)\n","    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n","    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n","    test_cat_agg.reset_index(inplace = True)\n","    # Transform float64 columns to float32\n","    cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n","    for col in tqdm(cols):\n","        test_num_agg[col] = test_num_agg[col].astype(np.float32)\n","    # Transform int64 columns to int32\n","    cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n","    for col in tqdm(cols):\n","        test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n","    # Get the difference\n","    test_diff = get_difference(test, num_features)\n","    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n","    del test_num_agg, test_cat_agg, test_diff\n","    gc.collect()\n","    # Save files to disk\n","    train.to_parquet(CFG.input_dir + 'train_fe.parquet')\n","    test.to_parquet(CFG.input_dir + 'test_fe.parquet')\n","\n","# Read & Preprocess Data\n","# read_preprocess_data()"]},{"cell_type":"markdown","metadata":{"id":"Wzx0k3PfVNZC"},"source":["# Training & Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["094c27799e904ad887bf502d9d5367e1","d7582498428741679f3245541de307af","0e455acbd2b7437989f9c2538eddf42c","d9a29e84eb26449f9c913b26c0abedb8","64f086d0f76b45689f450977d54c0c5a","bd22b7312ed6411b82357ebde7f69746","e23a67a309994fc4a8af398930ccb960","739ad9f40d26467ba2bb615ea1fffcfa","cbcf56b9ffd8432f9db2e147961b0109","4e910a6ebb4042ceb4cc1df1866f3ea0","70be1012bba3409db0325f7c1795bb00"]},"id":"sGIYhM_AVNZD","outputId":"e28cb6be-5118-4b3f-8f10-473d8e3f3078"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"094c27799e904ad887bf502d9d5367e1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1080 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"output_type":"stream","name":"stdout","text":[" \n","--------------------------------------------------\n","Training fold 0 with 1365 features...\n","[500]\ttraining's binary_logloss: 0.324285\ttraining's amex_metric: 0.784391\tvalid_1's binary_logloss: 0.328063\tvalid_1's amex_metric: 0.768083\n","[1000]\ttraining's binary_logloss: 0.241682\ttraining's amex_metric: 0.801132\tvalid_1's binary_logloss: 0.249719\tvalid_1's amex_metric: 0.778503\n","[1500]\ttraining's binary_logloss: 0.214901\ttraining's amex_metric: 0.817168\tvalid_1's binary_logloss: 0.228437\tvalid_1's amex_metric: 0.786161\n","[2000]\ttraining's binary_logloss: 0.201887\ttraining's amex_metric: 0.831792\tvalid_1's binary_logloss: 0.221463\tvalid_1's amex_metric: 0.789846\n","[2500]\ttraining's binary_logloss: 0.194335\ttraining's amex_metric: 0.843384\tvalid_1's binary_logloss: 0.219109\tvalid_1's amex_metric: 0.79339\n","[3000]\ttraining's binary_logloss: 0.18616\ttraining's amex_metric: 0.855988\tvalid_1's binary_logloss: 0.217012\tvalid_1's amex_metric: 0.794348\n","[3500]\ttraining's binary_logloss: 0.179909\ttraining's amex_metric: 0.866639\tvalid_1's binary_logloss: 0.215953\tvalid_1's amex_metric: 0.794853\n","[4000]\ttraining's binary_logloss: 0.173109\ttraining's amex_metric: 0.878385\tvalid_1's binary_logloss: 0.21506\tvalid_1's amex_metric: 0.796434\n","[4500]\ttraining's binary_logloss: 0.167888\ttraining's amex_metric: 0.887856\tvalid_1's binary_logloss: 0.2146\tvalid_1's amex_metric: 0.797417\n","[5000]\ttraining's binary_logloss: 0.163214\ttraining's amex_metric: 0.895865\tvalid_1's binary_logloss: 0.214281\tvalid_1's amex_metric: 0.798303\n","[5500]\ttraining's binary_logloss: 0.157829\ttraining's amex_metric: 0.90403\tvalid_1's binary_logloss: 0.213926\tvalid_1's amex_metric: 0.797845\n","[6000]\ttraining's binary_logloss: 0.152564\ttraining's amex_metric: 0.912214\tvalid_1's binary_logloss: 0.213561\tvalid_1's amex_metric: 0.798044\n","[6500]\ttraining's binary_logloss: 0.147837\ttraining's amex_metric: 0.920352\tvalid_1's binary_logloss: 0.213342\tvalid_1's amex_metric: 0.79827\n","[7000]\ttraining's binary_logloss: 0.143236\ttraining's amex_metric: 0.928005\tvalid_1's binary_logloss: 0.213112\tvalid_1's amex_metric: 0.798544\n","[7500]\ttraining's binary_logloss: 0.138474\ttraining's amex_metric: 0.935095\tvalid_1's binary_logloss: 0.212918\tvalid_1's amex_metric: 0.799598\n","[8000]\ttraining's binary_logloss: 0.134221\ttraining's amex_metric: 0.941433\tvalid_1's binary_logloss: 0.212779\tvalid_1's amex_metric: 0.799531\n","[8500]\ttraining's binary_logloss: 0.130382\ttraining's amex_metric: 0.947376\tvalid_1's binary_logloss: 0.212716\tvalid_1's amex_metric: 0.800349\n","[9000]\ttraining's binary_logloss: 0.126364\ttraining's amex_metric: 0.952775\tvalid_1's binary_logloss: 0.212611\tvalid_1's amex_metric: 0.800148\n","[9500]\ttraining's binary_logloss: 0.122934\ttraining's amex_metric: 0.958101\tvalid_1's binary_logloss: 0.212551\tvalid_1's amex_metric: 0.800253\n","[10000]\ttraining's binary_logloss: 0.119233\ttraining's amex_metric: 0.96283\tvalid_1's binary_logloss: 0.212492\tvalid_1's amex_metric: 0.799918\n","[10500]\ttraining's binary_logloss: 0.115108\ttraining's amex_metric: 0.967757\tvalid_1's binary_logloss: 0.212521\tvalid_1's amex_metric: 0.799453\n","[11000]\ttraining's binary_logloss: 0.111297\ttraining's amex_metric: 0.97238\tvalid_1's binary_logloss: 0.212544\tvalid_1's amex_metric: 0.79935\n","[11500]\ttraining's binary_logloss: 0.107981\ttraining's amex_metric: 0.976353\tvalid_1's binary_logloss: 0.212542\tvalid_1's amex_metric: 0.799637\n","[12000]\ttraining's binary_logloss: 0.105102\ttraining's amex_metric: 0.979266\tvalid_1's binary_logloss: 0.212624\tvalid_1's amex_metric: 0.799789\n","[12500]\ttraining's binary_logloss: 0.101882\ttraining's amex_metric: 0.982407\tvalid_1's binary_logloss: 0.212733\tvalid_1's amex_metric: 0.799793\n","[13000]\ttraining's binary_logloss: 0.099154\ttraining's amex_metric: 0.985065\tvalid_1's binary_logloss: 0.212799\tvalid_1's amex_metric: 0.799469\n","[13500]\ttraining's binary_logloss: 0.0960666\ttraining's amex_metric: 0.987555\tvalid_1's binary_logloss: 0.212861\tvalid_1's amex_metric: 0.799399\n","[14000]\ttraining's binary_logloss: 0.0933172\ttraining's amex_metric: 0.989602\tvalid_1's binary_logloss: 0.212925\tvalid_1's amex_metric: 0.79917\n","[14500]\ttraining's binary_logloss: 0.0905731\ttraining's amex_metric: 0.991412\tvalid_1's binary_logloss: 0.213023\tvalid_1's amex_metric: 0.799199\n","[15000]\ttraining's binary_logloss: 0.0879441\ttraining's amex_metric: 0.993176\tvalid_1's binary_logloss: 0.21315\tvalid_1's amex_metric: 0.799109\n","[15500]\ttraining's binary_logloss: 0.0851965\ttraining's amex_metric: 0.99457\tvalid_1's binary_logloss: 0.213262\tvalid_1's amex_metric: 0.799577\n","Our fold 0 CV score is 0.7995772474818776\n"," \n","--------------------------------------------------\n","Training fold 1 with 1365 features...\n","[500]\ttraining's binary_logloss: 0.323943\ttraining's amex_metric: 0.785144\tvalid_1's binary_logloss: 0.32842\tvalid_1's amex_metric: 0.768086\n","[1000]\ttraining's binary_logloss: 0.241045\ttraining's amex_metric: 0.801399\tvalid_1's binary_logloss: 0.250832\tvalid_1's amex_metric: 0.776553\n","[1500]\ttraining's binary_logloss: 0.214245\ttraining's amex_metric: 0.817989\tvalid_1's binary_logloss: 0.230204\tvalid_1's amex_metric: 0.782075\n","[2000]\ttraining's binary_logloss: 0.201172\ttraining's amex_metric: 0.833185\tvalid_1's binary_logloss: 0.223579\tvalid_1's amex_metric: 0.78601\n","[2500]\ttraining's binary_logloss: 0.193632\ttraining's amex_metric: 0.845057\tvalid_1's binary_logloss: 0.221284\tvalid_1's amex_metric: 0.789523\n","[3000]\ttraining's binary_logloss: 0.185421\ttraining's amex_metric: 0.856794\tvalid_1's binary_logloss: 0.219331\tvalid_1's amex_metric: 0.790271\n","[3500]\ttraining's binary_logloss: 0.17918\ttraining's amex_metric: 0.867275\tvalid_1's binary_logloss: 0.218411\tvalid_1's amex_metric: 0.792337\n","[4000]\ttraining's binary_logloss: 0.172332\ttraining's amex_metric: 0.878503\tvalid_1's binary_logloss: 0.217695\tvalid_1's amex_metric: 0.792334\n","[4500]\ttraining's binary_logloss: 0.16713\ttraining's amex_metric: 0.887999\tvalid_1's binary_logloss: 0.217337\tvalid_1's amex_metric: 0.792615\n","[5000]\ttraining's binary_logloss: 0.162419\ttraining's amex_metric: 0.896295\tvalid_1's binary_logloss: 0.216967\tvalid_1's amex_metric: 0.792975\n","[5500]\ttraining's binary_logloss: 0.157071\ttraining's amex_metric: 0.904416\tvalid_1's binary_logloss: 0.2166\tvalid_1's amex_metric: 0.793865\n","[6000]\ttraining's binary_logloss: 0.1518\ttraining's amex_metric: 0.913072\tvalid_1's binary_logloss: 0.21637\tvalid_1's amex_metric: 0.794324\n","[6500]\ttraining's binary_logloss: 0.147051\ttraining's amex_metric: 0.920725\tvalid_1's binary_logloss: 0.216219\tvalid_1's amex_metric: 0.793587\n","[7000]\ttraining's binary_logloss: 0.142492\ttraining's amex_metric: 0.928546\tvalid_1's binary_logloss: 0.216148\tvalid_1's amex_metric: 0.793609\n","[7500]\ttraining's binary_logloss: 0.137756\ttraining's amex_metric: 0.935813\tvalid_1's binary_logloss: 0.216062\tvalid_1's amex_metric: 0.794099\n","[8000]\ttraining's binary_logloss: 0.133463\ttraining's amex_metric: 0.942203\tvalid_1's binary_logloss: 0.216028\tvalid_1's amex_metric: 0.79388\n","[8500]\ttraining's binary_logloss: 0.129671\ttraining's amex_metric: 0.948322\tvalid_1's binary_logloss: 0.215988\tvalid_1's amex_metric: 0.793622\n","[9000]\ttraining's binary_logloss: 0.125683\ttraining's amex_metric: 0.953815\tvalid_1's binary_logloss: 0.216028\tvalid_1's amex_metric: 0.793744\n","[9500]\ttraining's binary_logloss: 0.122288\ttraining's amex_metric: 0.958954\tvalid_1's binary_logloss: 0.216007\tvalid_1's amex_metric: 0.794243\n","[10000]\ttraining's binary_logloss: 0.118607\ttraining's amex_metric: 0.963724\tvalid_1's binary_logloss: 0.21602\tvalid_1's amex_metric: 0.793814\n","[10500]\ttraining's binary_logloss: 0.114513\ttraining's amex_metric: 0.968652\tvalid_1's binary_logloss: 0.216149\tvalid_1's amex_metric: 0.793814\n","[11000]\ttraining's binary_logloss: 0.110714\ttraining's amex_metric: 0.973097\tvalid_1's binary_logloss: 0.216239\tvalid_1's amex_metric: 0.793071\n","[11500]\ttraining's binary_logloss: 0.107457\ttraining's amex_metric: 0.977114\tvalid_1's binary_logloss: 0.216265\tvalid_1's amex_metric: 0.793204\n","[12000]\ttraining's binary_logloss: 0.104582\ttraining's amex_metric: 0.979821\tvalid_1's binary_logloss: 0.216319\tvalid_1's amex_metric: 0.793798\n","[12500]\ttraining's binary_logloss: 0.10138\ttraining's amex_metric: 0.982715\tvalid_1's binary_logloss: 0.216344\tvalid_1's amex_metric: 0.793467\n","[13000]\ttraining's binary_logloss: 0.0986856\ttraining's amex_metric: 0.985236\tvalid_1's binary_logloss: 0.216413\tvalid_1's amex_metric: 0.793298\n","[13500]\ttraining's binary_logloss: 0.0955861\ttraining's amex_metric: 0.987573\tvalid_1's binary_logloss: 0.216585\tvalid_1's amex_metric: 0.793422\n","[14000]\ttraining's binary_logloss: 0.0928447\ttraining's amex_metric: 0.989847\tvalid_1's binary_logloss: 0.216695\tvalid_1's amex_metric: 0.793441\n","[14500]\ttraining's binary_logloss: 0.0901208\ttraining's amex_metric: 0.991716\tvalid_1's binary_logloss: 0.216789\tvalid_1's amex_metric: 0.794004\n","[15000]\ttraining's binary_logloss: 0.0874841\ttraining's amex_metric: 0.993215\tvalid_1's binary_logloss: 0.216919\tvalid_1's amex_metric: 0.794291\n","[15500]\ttraining's binary_logloss: 0.0847657\ttraining's amex_metric: 0.994738\tvalid_1's binary_logloss: 0.217064\tvalid_1's amex_metric: 0.794264\n","Our fold 1 CV score is 0.7942644990295504\n"," \n","--------------------------------------------------\n","Training fold 2 with 1365 features...\n","[500]\ttraining's binary_logloss: 0.324285\ttraining's amex_metric: 0.78434\tvalid_1's binary_logloss: 0.327627\tvalid_1's amex_metric: 0.771931\n","[1000]\ttraining's binary_logloss: 0.24164\ttraining's amex_metric: 0.800965\tvalid_1's binary_logloss: 0.249381\tvalid_1's amex_metric: 0.780897\n","[1500]\ttraining's binary_logloss: 0.214912\ttraining's amex_metric: 0.817359\tvalid_1's binary_logloss: 0.228159\tvalid_1's amex_metric: 0.786442\n","[2000]\ttraining's binary_logloss: 0.201882\ttraining's amex_metric: 0.831818\tvalid_1's binary_logloss: 0.221353\tvalid_1's amex_metric: 0.789993\n","[2500]\ttraining's binary_logloss: 0.19429\ttraining's amex_metric: 0.843667\tvalid_1's binary_logloss: 0.219081\tvalid_1's amex_metric: 0.79331\n","[3000]\ttraining's binary_logloss: 0.186061\ttraining's amex_metric: 0.855657\tvalid_1's binary_logloss: 0.217014\tvalid_1's amex_metric: 0.794572\n","[3500]\ttraining's binary_logloss: 0.179804\ttraining's amex_metric: 0.866121\tvalid_1's binary_logloss: 0.216051\tvalid_1's amex_metric: 0.795323\n","[4000]\ttraining's binary_logloss: 0.172953\ttraining's amex_metric: 0.877186\tvalid_1's binary_logloss: 0.215224\tvalid_1's amex_metric: 0.796248\n","[4500]\ttraining's binary_logloss: 0.167691\ttraining's amex_metric: 0.886806\tvalid_1's binary_logloss: 0.214776\tvalid_1's amex_metric: 0.797486\n","[5000]\ttraining's binary_logloss: 0.163002\ttraining's amex_metric: 0.894843\tvalid_1's binary_logloss: 0.214512\tvalid_1's amex_metric: 0.797625\n","[5500]\ttraining's binary_logloss: 0.157644\ttraining's amex_metric: 0.903758\tvalid_1's binary_logloss: 0.214136\tvalid_1's amex_metric: 0.79869\n","[6000]\ttraining's binary_logloss: 0.152372\ttraining's amex_metric: 0.912378\tvalid_1's binary_logloss: 0.213864\tvalid_1's amex_metric: 0.799045\n","[6500]\ttraining's binary_logloss: 0.147592\ttraining's amex_metric: 0.920651\tvalid_1's binary_logloss: 0.213651\tvalid_1's amex_metric: 0.79868\n","[7000]\ttraining's binary_logloss: 0.142997\ttraining's amex_metric: 0.928449\tvalid_1's binary_logloss: 0.213505\tvalid_1's amex_metric: 0.79811\n","[7500]\ttraining's binary_logloss: 0.138247\ttraining's amex_metric: 0.935409\tvalid_1's binary_logloss: 0.213409\tvalid_1's amex_metric: 0.798066\n","[8000]\ttraining's binary_logloss: 0.133937\ttraining's amex_metric: 0.941856\tvalid_1's binary_logloss: 0.213344\tvalid_1's amex_metric: 0.798023\n","[8500]\ttraining's binary_logloss: 0.130121\ttraining's amex_metric: 0.947837\tvalid_1's binary_logloss: 0.213236\tvalid_1's amex_metric: 0.798252\n","[9000]\ttraining's binary_logloss: 0.126104\ttraining's amex_metric: 0.953226\tvalid_1's binary_logloss: 0.213174\tvalid_1's amex_metric: 0.7984\n","[9500]\ttraining's binary_logloss: 0.1227\ttraining's amex_metric: 0.958255\tvalid_1's binary_logloss: 0.213115\tvalid_1's amex_metric: 0.798654\n","[10000]\ttraining's binary_logloss: 0.119031\ttraining's amex_metric: 0.963108\tvalid_1's binary_logloss: 0.213191\tvalid_1's amex_metric: 0.798106\n","[10500]\ttraining's binary_logloss: 0.114913\ttraining's amex_metric: 0.968045\tvalid_1's binary_logloss: 0.213218\tvalid_1's amex_metric: 0.7979\n","[11000]\ttraining's binary_logloss: 0.111114\ttraining's amex_metric: 0.972599\tvalid_1's binary_logloss: 0.213187\tvalid_1's amex_metric: 0.798302\n","[11500]\ttraining's binary_logloss: 0.107831\ttraining's amex_metric: 0.976337\tvalid_1's binary_logloss: 0.213256\tvalid_1's amex_metric: 0.797812\n","[12000]\ttraining's binary_logloss: 0.10494\ttraining's amex_metric: 0.979386\tvalid_1's binary_logloss: 0.213324\tvalid_1's amex_metric: 0.798081\n","[12500]\ttraining's binary_logloss: 0.101742\ttraining's amex_metric: 0.982532\tvalid_1's binary_logloss: 0.213401\tvalid_1's amex_metric: 0.797934\n","[13000]\ttraining's binary_logloss: 0.0990165\ttraining's amex_metric: 0.985189\tvalid_1's binary_logloss: 0.213461\tvalid_1's amex_metric: 0.798062\n","[13500]\ttraining's binary_logloss: 0.0959226\ttraining's amex_metric: 0.987833\tvalid_1's binary_logloss: 0.213618\tvalid_1's amex_metric: 0.797669\n","[14000]\ttraining's binary_logloss: 0.0931878\ttraining's amex_metric: 0.989919\tvalid_1's binary_logloss: 0.213726\tvalid_1's amex_metric: 0.798445\n","[14500]\ttraining's binary_logloss: 0.0904736\ttraining's amex_metric: 0.991691\tvalid_1's binary_logloss: 0.213865\tvalid_1's amex_metric: 0.797676\n","[15000]\ttraining's binary_logloss: 0.0878469\ttraining's amex_metric: 0.993262\tvalid_1's binary_logloss: 0.21395\tvalid_1's amex_metric: 0.797986\n","[15500]\ttraining's binary_logloss: 0.0851044\ttraining's amex_metric: 0.994757\tvalid_1's binary_logloss: 0.214097\tvalid_1's amex_metric: 0.79893\n","Our fold 2 CV score is 0.7989302896636366\n"," \n","--------------------------------------------------\n","Training fold 3 with 1365 features...\n","[500]\ttraining's binary_logloss: 0.323966\ttraining's amex_metric: 0.785471\tvalid_1's binary_logloss: 0.329035\tvalid_1's amex_metric: 0.766259\n","[1000]\ttraining's binary_logloss: 0.241133\ttraining's amex_metric: 0.801583\tvalid_1's binary_logloss: 0.25136\tvalid_1's amex_metric: 0.775933\n","[1500]\ttraining's binary_logloss: 0.214301\ttraining's amex_metric: 0.818258\tvalid_1's binary_logloss: 0.230378\tvalid_1's amex_metric: 0.78263\n","[2000]\ttraining's binary_logloss: 0.201303\ttraining's amex_metric: 0.83274\tvalid_1's binary_logloss: 0.223654\tvalid_1's amex_metric: 0.786643\n","[2500]\ttraining's binary_logloss: 0.193753\ttraining's amex_metric: 0.84443\tvalid_1's binary_logloss: 0.221343\tvalid_1's amex_metric: 0.789141\n","[3000]\ttraining's binary_logloss: 0.185593\ttraining's amex_metric: 0.856507\tvalid_1's binary_logloss: 0.219319\tvalid_1's amex_metric: 0.789937\n","[3500]\ttraining's binary_logloss: 0.179331\ttraining's amex_metric: 0.867246\tvalid_1's binary_logloss: 0.21835\tvalid_1's amex_metric: 0.790485\n","[4000]\ttraining's binary_logloss: 0.172492\ttraining's amex_metric: 0.878377\tvalid_1's binary_logloss: 0.217503\tvalid_1's amex_metric: 0.791856\n","[4500]\ttraining's binary_logloss: 0.167215\ttraining's amex_metric: 0.888062\tvalid_1's binary_logloss: 0.217137\tvalid_1's amex_metric: 0.79241\n","[5000]\ttraining's binary_logloss: 0.162527\ttraining's amex_metric: 0.896125\tvalid_1's binary_logloss: 0.216832\tvalid_1's amex_metric: 0.79282\n","[5500]\ttraining's binary_logloss: 0.157158\ttraining's amex_metric: 0.904419\tvalid_1's binary_logloss: 0.216412\tvalid_1's amex_metric: 0.793829\n","[6000]\ttraining's binary_logloss: 0.151875\ttraining's amex_metric: 0.913332\tvalid_1's binary_logloss: 0.216046\tvalid_1's amex_metric: 0.794064\n","[6500]\ttraining's binary_logloss: 0.147113\ttraining's amex_metric: 0.921018\tvalid_1's binary_logloss: 0.215818\tvalid_1's amex_metric: 0.794005\n","[7000]\ttraining's binary_logloss: 0.142537\ttraining's amex_metric: 0.928622\tvalid_1's binary_logloss: 0.215699\tvalid_1's amex_metric: 0.794564\n","[7500]\ttraining's binary_logloss: 0.137787\ttraining's amex_metric: 0.93564\tvalid_1's binary_logloss: 0.215549\tvalid_1's amex_metric: 0.794394\n","[8000]\ttraining's binary_logloss: 0.133505\ttraining's amex_metric: 0.94214\tvalid_1's binary_logloss: 0.215539\tvalid_1's amex_metric: 0.793735\n","[8500]\ttraining's binary_logloss: 0.129676\ttraining's amex_metric: 0.948057\tvalid_1's binary_logloss: 0.215469\tvalid_1's amex_metric: 0.793568\n","[9000]\ttraining's binary_logloss: 0.125686\ttraining's amex_metric: 0.953504\tvalid_1's binary_logloss: 0.215366\tvalid_1's amex_metric: 0.7939\n","[9500]\ttraining's binary_logloss: 0.122314\ttraining's amex_metric: 0.95836\tvalid_1's binary_logloss: 0.215345\tvalid_1's amex_metric: 0.794242\n","[10000]\ttraining's binary_logloss: 0.118611\ttraining's amex_metric: 0.963077\tvalid_1's binary_logloss: 0.215332\tvalid_1's amex_metric: 0.793962\n","[10500]\ttraining's binary_logloss: 0.114525\ttraining's amex_metric: 0.968126\tvalid_1's binary_logloss: 0.215347\tvalid_1's amex_metric: 0.793805\n","[11000]\ttraining's binary_logloss: 0.110712\ttraining's amex_metric: 0.972714\tvalid_1's binary_logloss: 0.21533\tvalid_1's amex_metric: 0.793433\n","[11500]\ttraining's binary_logloss: 0.107424\ttraining's amex_metric: 0.976759\tvalid_1's binary_logloss: 0.215336\tvalid_1's amex_metric: 0.793757\n","[12000]\ttraining's binary_logloss: 0.104562\ttraining's amex_metric: 0.979827\tvalid_1's binary_logloss: 0.21538\tvalid_1's amex_metric: 0.794456\n","[12500]\ttraining's binary_logloss: 0.101374\ttraining's amex_metric: 0.982769\tvalid_1's binary_logloss: 0.215405\tvalid_1's amex_metric: 0.794544\n","[13000]\ttraining's binary_logloss: 0.0986688\ttraining's amex_metric: 0.985344\tvalid_1's binary_logloss: 0.215485\tvalid_1's amex_metric: 0.794391\n","[13500]\ttraining's binary_logloss: 0.095574\ttraining's amex_metric: 0.987784\tvalid_1's binary_logloss: 0.21565\tvalid_1's amex_metric: 0.794168\n","[14000]\ttraining's binary_logloss: 0.092835\ttraining's amex_metric: 0.989867\tvalid_1's binary_logloss: 0.215782\tvalid_1's amex_metric: 0.794276\n","[14500]\ttraining's binary_logloss: 0.0901185\ttraining's amex_metric: 0.991708\tvalid_1's binary_logloss: 0.215949\tvalid_1's amex_metric: 0.794113\n","[15000]\ttraining's binary_logloss: 0.0875005\ttraining's amex_metric: 0.993282\tvalid_1's binary_logloss: 0.216049\tvalid_1's amex_metric: 0.793676\n","[15500]\ttraining's binary_logloss: 0.0847678\ttraining's amex_metric: 0.994759\tvalid_1's binary_logloss: 0.216148\tvalid_1's amex_metric: 0.794003\n","Our fold 3 CV score is 0.7940030769961366\n"," \n","--------------------------------------------------\n","Training fold 4 with 1365 features...\n","[500]\ttraining's binary_logloss: 0.324039\ttraining's amex_metric: 0.784544\tvalid_1's binary_logloss: 0.32849\tvalid_1's amex_metric: 0.770405\n","[1000]\ttraining's binary_logloss: 0.241469\ttraining's amex_metric: 0.800468\tvalid_1's binary_logloss: 0.250526\tvalid_1's amex_metric: 0.779922\n","[1500]\ttraining's binary_logloss: 0.214649\ttraining's amex_metric: 0.816336\tvalid_1's binary_logloss: 0.229212\tvalid_1's amex_metric: 0.788513\n","[2000]\ttraining's binary_logloss: 0.201624\ttraining's amex_metric: 0.831221\tvalid_1's binary_logloss: 0.222383\tvalid_1's amex_metric: 0.793207\n","[2500]\ttraining's binary_logloss: 0.193999\ttraining's amex_metric: 0.843493\tvalid_1's binary_logloss: 0.220095\tvalid_1's amex_metric: 0.794978\n","[3000]\ttraining's binary_logloss: 0.185753\ttraining's amex_metric: 0.856058\tvalid_1's binary_logloss: 0.218037\tvalid_1's amex_metric: 0.795666\n","[3500]\ttraining's binary_logloss: 0.179486\ttraining's amex_metric: 0.866764\tvalid_1's binary_logloss: 0.217001\tvalid_1's amex_metric: 0.796848\n","[4000]\ttraining's binary_logloss: 0.172594\ttraining's amex_metric: 0.877976\tvalid_1's binary_logloss: 0.216101\tvalid_1's amex_metric: 0.7974\n","[4500]\ttraining's binary_logloss: 0.16733\ttraining's amex_metric: 0.887644\tvalid_1's binary_logloss: 0.215661\tvalid_1's amex_metric: 0.797819\n","[5000]\ttraining's binary_logloss: 0.162628\ttraining's amex_metric: 0.895771\tvalid_1's binary_logloss: 0.215396\tvalid_1's amex_metric: 0.797932\n","[5500]\ttraining's binary_logloss: 0.157286\ttraining's amex_metric: 0.904219\tvalid_1's binary_logloss: 0.215056\tvalid_1's amex_metric: 0.797645\n","[6000]\ttraining's binary_logloss: 0.152005\ttraining's amex_metric: 0.91274\tvalid_1's binary_logloss: 0.214806\tvalid_1's amex_metric: 0.797616\n","[6500]\ttraining's binary_logloss: 0.147282\ttraining's amex_metric: 0.920446\tvalid_1's binary_logloss: 0.21455\tvalid_1's amex_metric: 0.797858\n","[7000]\ttraining's binary_logloss: 0.142724\ttraining's amex_metric: 0.928388\tvalid_1's binary_logloss: 0.214366\tvalid_1's amex_metric: 0.798774\n","[7500]\ttraining's binary_logloss: 0.137987\ttraining's amex_metric: 0.935631\tvalid_1's binary_logloss: 0.214222\tvalid_1's amex_metric: 0.798703\n","[8000]\ttraining's binary_logloss: 0.133703\ttraining's amex_metric: 0.941823\tvalid_1's binary_logloss: 0.214144\tvalid_1's amex_metric: 0.799523\n","[8500]\ttraining's binary_logloss: 0.1299\ttraining's amex_metric: 0.948162\tvalid_1's binary_logloss: 0.214086\tvalid_1's amex_metric: 0.799164\n","[9000]\ttraining's binary_logloss: 0.125934\ttraining's amex_metric: 0.953649\tvalid_1's binary_logloss: 0.214055\tvalid_1's amex_metric: 0.799849\n","[9500]\ttraining's binary_logloss: 0.122556\ttraining's amex_metric: 0.958508\tvalid_1's binary_logloss: 0.214053\tvalid_1's amex_metric: 0.799009\n","[10000]\ttraining's binary_logloss: 0.118878\ttraining's amex_metric: 0.963148\tvalid_1's binary_logloss: 0.214043\tvalid_1's amex_metric: 0.798785\n","[10500]\ttraining's binary_logloss: 0.114766\ttraining's amex_metric: 0.968074\tvalid_1's binary_logloss: 0.214023\tvalid_1's amex_metric: 0.799288\n","[11000]\ttraining's binary_logloss: 0.110994\ttraining's amex_metric: 0.972529\tvalid_1's binary_logloss: 0.214011\tvalid_1's amex_metric: 0.798699\n","[11500]\ttraining's binary_logloss: 0.107703\ttraining's amex_metric: 0.976327\tvalid_1's binary_logloss: 0.214066\tvalid_1's amex_metric: 0.799056\n","[12000]\ttraining's binary_logloss: 0.104808\ttraining's amex_metric: 0.979396\tvalid_1's binary_logloss: 0.214152\tvalid_1's amex_metric: 0.7986\n","[12500]\ttraining's binary_logloss: 0.101617\ttraining's amex_metric: 0.982485\tvalid_1's binary_logloss: 0.214203\tvalid_1's amex_metric: 0.798733\n","[13000]\ttraining's binary_logloss: 0.098894\ttraining's amex_metric: 0.985119\tvalid_1's binary_logloss: 0.21428\tvalid_1's amex_metric: 0.79814\n","[13500]\ttraining's binary_logloss: 0.0958002\ttraining's amex_metric: 0.987487\tvalid_1's binary_logloss: 0.214336\tvalid_1's amex_metric: 0.798474\n","[14000]\ttraining's binary_logloss: 0.0930582\ttraining's amex_metric: 0.989545\tvalid_1's binary_logloss: 0.214442\tvalid_1's amex_metric: 0.798742\n","[14500]\ttraining's binary_logloss: 0.0903399\ttraining's amex_metric: 0.991506\tvalid_1's binary_logloss: 0.214572\tvalid_1's amex_metric: 0.79903\n","[15000]\ttraining's binary_logloss: 0.0877315\ttraining's amex_metric: 0.993168\tvalid_1's binary_logloss: 0.214735\tvalid_1's amex_metric: 0.798996\n","[15500]\ttraining's binary_logloss: 0.0850027\ttraining's amex_metric: 0.994588\tvalid_1's binary_logloss: 0.214876\tvalid_1's amex_metric: 0.798849\n"]}],"source":["# ====================================================\n","# Library\n","# ====================================================\n","import os\n","import gc\n","import warnings\n","warnings.filterwarnings('ignore')\n","import random\n","import scipy as sp\n","import numpy as np\n","import pandas as pd\n","import joblib\n","import itertools\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","from tqdm.auto import tqdm\n","from sklearn.model_selection import StratifiedKFold, train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import lightgbm as lgb\n","from itertools import combinations\n","\n","\n","\n","# ====================================================\n","# Seed everything\n","# ====================================================\n","def seed_everything(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","\n","# ====================================================\n","# Read data\n","# ====================================================\n","def read_data():\n","    train = pd.read_parquet(CFG.input_dir + 'train_fe.parquet')\n","    test = pd.read_parquet(CFG.input_dir + 'test_fe.parquet')\n","    return train, test\n","\n","# ====================================================\n","# Amex metric\n","# ====================================================\n","def amex_metric(y_true, y_pred):\n","    labels = np.transpose(np.array([y_true, y_pred]))\n","    labels = labels[labels[:, 1].argsort()[::-1]]\n","    weights = np.where(labels[:,0]==0, 20, 1)\n","    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n","    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n","    gini = [0,0]\n","    for i in [1,0]:\n","        labels = np.transpose(np.array([y_true, y_pred]))\n","        labels = labels[labels[:, i].argsort()[::-1]]\n","        weight = np.where(labels[:,0]==0, 20, 1)\n","        weight_random = np.cumsum(weight / np.sum(weight))\n","        total_pos = np.sum(labels[:, 0] *  weight)\n","        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n","        lorentz = cum_pos_found / total_pos\n","        gini[i] = np.sum((lorentz - weight_random) * weight)\n","    return 0.5 * (gini[1]/gini[0] + top_four)\n","\n","# ====================================================\n","# LGBM amex metric\n","# ====================================================\n","def lgb_amex_metric(y_pred, y_true):\n","    y_true = y_true.get_label()\n","    return 'amex_metric', amex_metric(y_true, y_pred), True\n","\n","# ====================================================\n","# Train & Evaluate\n","# ====================================================\n","def train_and_evaluate(train, test):\n","    # Label encode categorical features\n","    cat_features = [\n","        \"B_30\",\n","        \"B_38\",\n","        \"D_114\",\n","        \"D_116\",\n","        \"D_117\",\n","        \"D_120\",\n","        \"D_126\",\n","        \"D_63\",\n","        \"D_64\",\n","        \"D_66\",\n","        \"D_68\"\n","    ]\n","    cat_features = [f\"{cf}_last\" for cf in cat_features]\n","    for cat_col in cat_features:\n","        encoder = LabelEncoder()\n","        train[cat_col] = encoder.fit_transform(train[cat_col])\n","        test[cat_col] = encoder.transform(test[cat_col])\n","    # Round last float features to 2 decimal place\n","    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n","    num_cols = [col for col in num_cols if 'last' in col]\n","    for col in num_cols:\n","        train[col + '_round2'] = train[col].round(2)\n","        test[col + '_round2'] = test[col].round(2)\n","    # Get the difference between last and mean\n","    num_cols = [col for col in train.columns if 'last' in col]\n","    num_cols = [col[:-5] for col in num_cols if 'round' not in col]\n","    for col in num_cols:\n","        try:\n","            train[f'{col}_last_mean_diff'] = train[f'{col}_last'] - train[f'{col}_mean']\n","            test[f'{col}_last_mean_diff'] = test[f'{col}_last'] - test[f'{col}_mean']\n","        except:\n","            pass\n","    # Transform float64 and float32 to float16\n","    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n","    for col in tqdm(num_cols):\n","        train[col] = train[col].astype(np.float16)\n","        test[col] = test[col].astype(np.float16)\n","    # Get feature list\n","    features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n","    params = {\n","        'objective': 'binary',\n","        'metric': CFG.metric,\n","        'boosting': CFG.boosting_type,\n","        'seed': CFG.seed,\n","        'num_leaves': 120,#100,\n","        'learning_rate': 0.01,\n","        'feature_fraction': 0.20,\n","        'bagging_freq': 10,\n","        'bagging_fraction': 0.50,\n","        'n_jobs': -1,\n","        'lambda_l2': 2,\n","        'min_data_in_leaf': 40,\n","        }\n","    # Create a numpy array to store test predictions\n","    test_predictions = np.zeros(len(test))\n","    # Create a numpy array to store out of folds predictions\n","    oof_predictions = np.zeros(len(train))\n","    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n","    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n","        print(' ')\n","        print('-'*50)\n","        print(f'Training fold {fold} with {len(features)} features...')\n","        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n","        y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n","        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n","        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n","        model = lgb.train(\n","            params = params,\n","            train_set = lgb_train,\n","            num_boost_round = 15500,\n","            valid_sets = [lgb_train, lgb_valid],\n","            early_stopping_rounds = 1500,\n","            verbose_eval = 500,\n","            feval = lgb_amex_metric\n","            )\n","        # Save best model\n","#         joblib.dump(model, f'/content/drive/MyDrive/Amex/Models/lgbm_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n","        joblib.dump(model, f'{CFG.input_dir}lgbm_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n","        \n","        # Predict validation\n","        val_pred = model.predict(x_val)\n","        # Add to out of folds array\n","        oof_predictions[val_ind] = val_pred\n","        # Predict the test set\n","        test_pred = model.predict(test[features])\n","        test_predictions += test_pred / CFG.n_folds\n","        # Compute fold metric\n","        score = amex_metric(y_val, val_pred)\n","        print(f'Our fold {fold} CV score is {score}')\n","        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n","        gc.collect()\n","    # Compute out of folds metric\n","    score = amex_metric(train[CFG.target], oof_predictions)\n","    print(f'Our out of folds CV score is {score}')\n","    # Create a dataframe to store out of folds predictions\n","    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n","#     oof_df.to_csv(f'/content/drive/MyDrive/Amex/OOF/oof_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n","    oof_df.to_csv(f'{CFG.input_dir}oof_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n","    \n","    # Create a dataframe to store test prediction\n","    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n","#     test_df.to_csv(f'/content/drive/MyDrive/Amex/Predictions/test_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n","    test_df.to_csv(f'{CFG.input_dir}test_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n","    \n","seed_everything(CFG.seed)\n","train, test = read_data()\n","train_and_evaluate(train, test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H4h4IGSdZByf"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"hQKlH5vuVNZF"},"source":["# Read Submission File\n","This is the submission file corresponding to the output of the previous pipeline (using the average blend of 3 seeds)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXqsUUenVNZF"},"outputs":[],"source":["# sub = pd.read_csv('../input/amex-sub/test_lgbm_baseline_5fold_seed_blend.csv')\n","# sub.to_csv('test_lgbm_baseline_5fold_seed_blend.csv', index = False)"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"amex-lgbm-dart-cv-0-7977.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"094c27799e904ad887bf502d9d5367e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d7582498428741679f3245541de307af","IPY_MODEL_0e455acbd2b7437989f9c2538eddf42c","IPY_MODEL_d9a29e84eb26449f9c913b26c0abedb8"],"layout":"IPY_MODEL_64f086d0f76b45689f450977d54c0c5a"}},"0e455acbd2b7437989f9c2538eddf42c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_739ad9f40d26467ba2bb615ea1fffcfa","max":1080,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cbcf56b9ffd8432f9db2e147961b0109","value":1080}},"4e910a6ebb4042ceb4cc1df1866f3ea0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64f086d0f76b45689f450977d54c0c5a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70be1012bba3409db0325f7c1795bb00":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"739ad9f40d26467ba2bb615ea1fffcfa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd22b7312ed6411b82357ebde7f69746":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbcf56b9ffd8432f9db2e147961b0109":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d7582498428741679f3245541de307af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd22b7312ed6411b82357ebde7f69746","placeholder":"​","style":"IPY_MODEL_e23a67a309994fc4a8af398930ccb960","value":"100%"}},"d9a29e84eb26449f9c913b26c0abedb8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e910a6ebb4042ceb4cc1df1866f3ea0","placeholder":"​","style":"IPY_MODEL_70be1012bba3409db0325f7c1795bb00","value":" 1080/1080 [06:30&lt;00:00, 102.80it/s]"}},"e23a67a309994fc4a8af398930ccb960":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
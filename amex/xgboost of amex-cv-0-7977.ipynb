{"cells":[{"cell_type":"markdown","metadata":{"id":"V8TX5ncPVNY3"},"source":["# Comments:\n","    \n","This is an improvement of my baseline, you can find it here: https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7963\n","\n","The main difference between this solution and previous one is that we add new features and do seed blend to boost LB. Single 5 kfold model using seed 42 achieve an out of folds CV of 0.7977 and a public leaderboard of 0.799. If we use seed blend (train three different models using seed 42, 52, 62 and then average predictions) the LB boost niceley.\n","\n","The main features that boost CV are the following:\n","\n","* The difference between last value and the lag1\n","* The difference between last value and the average (this features gives a nice boost)\n","\n","This feature engineer is done on all the last columns, so we actually add a lot of features, this model used 1368 features.\n","\n","I uploaded test predictions to avoid running training and inference\n","\n","Next Steps:\n","\n","* Could try feature selection, maybe a lot of the feature are just noise, actually I perform permutation importance and I reduce the amount of features to 1000 app and the CV was almost the same. Maybe there is a better feature selection technique that can boost performance.\n","\n","* Could try different models, maybe some neural network with the same features or a subset of the features and then blend with LGBM can work, in my experience blending tree models and neural network works great because they are very diverse so the boost is nice\n","\n","* Could try more feature engineering, maybe we can create more features that extract the hidden signal of the dataset, actually I would first work on this option and really try to capture all the signal that the dataset has."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658329669087,"user":{"displayName":"darwin zhao","userId":"12087337285275609974"},"user_tz":-480},"id":"U38gJrkJVPTd"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"oem3LtDHVNY_"},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1989,"status":"ok","timestamp":1658329671069,"user":{"displayName":"darwin zhao","userId":"12087337285275609974"},"user_tz":-480},"id":"RQ1JQbKOVNZA"},"outputs":[],"source":["# ====================================================\n","# Configurations\n","# ====================================================\n","class CFG:\n","    input_dir = '/content/drive/MyDrive/amex/output/7977xgb/'\n","    data_dir='/content/drive/MyDrive/amex/output/7977o/'\n","    seed = 103\n","    n_folds = 5\n","    target = 'target'\n","    boosting_type = 'dart'\n","    metric = 'binary_logloss'\n","# ====================================================\n","# Library\n","# ====================================================\n","import gc\n","import warnings\n","warnings.filterwarnings('ignore')\n","import scipy as sp\n","import numpy as np\n","import pandas as pd\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","from tqdm.auto import tqdm\n","import itertools\n","\n","# !pip install catboost\n","# from catboost import CatBoostClassifier\n","import xgboost as xgb\n","\n","\n","class CB_Amex_Metric(object):\n","    def get_final_error(self, error, weight):\n","        return error\n","    def is_max_optimal(self):\n","        return True\n","    def evaluate(self, y_pred, y_true, weight):\n","        y_pred = y_pred[0]\n","        indices = np.argsort(y_pred)[::-1]\n","        preds, target = y_pred[indices], y_true[indices]\n","        weight = 20.0 - target * 19.0\n","        cum_norm_weight = (weight / weight.sum()).cumsum()\n","        four_pct_mask = cum_norm_weight \u003c= 0.04\n","        d = np.sum(target[four_pct_mask]) / np.sum(target)\n","        weighted_target = target * weight\n","        lorentz = (weighted_target / weighted_target.sum()).cumsum()\n","        gini = ((lorentz - cum_norm_weight) * weight).sum()\n","        n_pos = np.sum(target)\n","        n_neg = target.shape[0] - n_pos\n","        gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n","        g = gini / gini_max\n","        return 0.5 * (g + d), 0\n","\n","# ====================================================\n","# Get the difference\n","# ====================================================\n","def get_difference(data, num_features):\n","    df1 = []\n","    customer_ids = []\n","    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n","        # Get the differences\n","        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n","        # Append to lists\n","        df1.append(diff_df1)\n","        customer_ids.append(customer_id)\n","    # Concatenate\n","    df1 = np.concatenate(df1, axis = 0)\n","    # Transform to dataframe\n","    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n","    # Add customer id\n","    df1['customer_ID'] = customer_ids\n","    return df1\n","\n","# ====================================================\n","# Read \u0026 preprocess data and save it to disk\n","# ====================================================\n","def read_preprocess_data():\n","    train = pd.read_parquet('/content/drive/MyDrive/amex/data/amex-data-integer-dtypes-parquet-format/train.parquet')\n","    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n","    cat_features = [\n","        \"B_30\",\n","        \"B_38\",\n","        \"D_114\",\n","        \"D_116\",\n","        \"D_117\",\n","        \"D_120\",\n","        \"D_126\",\n","        \"D_63\",\n","        \"D_64\",\n","        \"D_66\",\n","        \"D_68\",\n","    ]\n","    num_features = [col for col in features if col not in cat_features]\n","    print('Starting training feature engineer...')\n","    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n","    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n","    train_num_agg.reset_index(inplace = True)\n","    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n","    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n","    train_cat_agg.reset_index(inplace = True)\n","    train_labels = pd.read_csv('/content/drive/MyDrive/amex/data/train_labels.csv')\n","    # Transform float64 columns to float32\n","    cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n","    for col in tqdm(cols):\n","        train_num_agg[col] = train_num_agg[col].astype(np.float32)\n","    # Transform int64 columns to int32\n","    cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n","    for col in tqdm(cols):\n","        train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n","    # Get the difference\n","    train_diff = get_difference(train, num_features)\n","    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n","    del train_num_agg, train_cat_agg, train_diff\n","    gc.collect()\n","    test = pd.read_parquet('/content/drive/MyDrive/amex/data/amex-data-integer-dtypes-parquet-format/test.parquet')\n","    print('Starting test feature engineer...')\n","    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n","    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n","    test_num_agg.reset_index(inplace = True)\n","    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n","    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n","    test_cat_agg.reset_index(inplace = True)\n","    # Transform float64 columns to float32\n","    cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n","    for col in tqdm(cols):\n","        test_num_agg[col] = test_num_agg[col].astype(np.float32)\n","    # Transform int64 columns to int32\n","    cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n","    for col in tqdm(cols):\n","        test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n","    # Get the difference\n","    test_diff = get_difference(test, num_features)\n","    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n","    del test_num_agg, test_cat_agg, test_diff\n","    gc.collect()\n","    # Save files to disk\n","    train.to_parquet(CFG.data_dir + 'train_fe.parquet')\n","    test.to_parquet(CFG.data_dir + 'test_fe.parquet')\n","\n","# Read \u0026 Preprocess Data\n","# read_preprocess_data()"]},{"cell_type":"markdown","metadata":{"id":"Wzx0k3PfVNZC"},"source":["# Training \u0026 Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":49},"id":"sGIYhM_AVNZD"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5679eb42c42740f48c05ad35672f008e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1080 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":[" \n","--------------------------------------------------\n","Training fold 0 with 1365 features...\n","[0]\ttrain-logloss:0.686779\teval-logloss:0.686783\ttrain-amex:0.6822\teval-amex:0.679253\n","Multiple eval metrics have been passed: 'eval-amex' will be used for early stopping.\n","\n","Will train until eval-amex hasn't improved in 5000 rounds.\n","[500]\ttrain-logloss:0.236637\teval-logloss:0.237832\ttrain-amex:0.769628\teval-amex:0.768287\n","[1000]\ttrain-logloss:0.221646\teval-logloss:0.225187\ttrain-amex:0.7874\teval-amex:0.780953\n","[1500]\ttrain-logloss:0.215238\teval-logloss:0.221099\ttrain-amex:0.796127\teval-amex:0.786629\n","[2000]\ttrain-logloss:0.210983\teval-logloss:0.219066\ttrain-amex:0.802938\teval-amex:0.790493\n","[2500]\ttrain-logloss:0.207566\teval-logloss:0.217771\ttrain-amex:0.808475\teval-amex:0.791512\n","[3000]\ttrain-logloss:0.204594\teval-logloss:0.216868\ttrain-amex:0.812872\teval-amex:0.792819\n","[3500]\ttrain-logloss:0.20187\teval-logloss:0.216257\ttrain-amex:0.817403\teval-amex:0.79358\n","[4000]\ttrain-logloss:0.199376\teval-logloss:0.215818\ttrain-amex:0.821639\teval-amex:0.794156\n","[4500]\ttrain-logloss:0.196987\teval-logloss:0.215436\ttrain-amex:0.825859\teval-amex:0.794165\n","[5000]\ttrain-logloss:0.194748\teval-logloss:0.215184\ttrain-amex:0.82976\teval-amex:0.794044\n","[5500]\ttrain-logloss:0.192614\teval-logloss:0.214978\ttrain-amex:0.833294\teval-amex:0.794495\n","[6000]\ttrain-logloss:0.190499\teval-logloss:0.214788\ttrain-amex:0.837074\teval-amex:0.794414\n","[6500]\ttrain-logloss:0.188437\teval-logloss:0.214642\ttrain-amex:0.840865\teval-amex:0.794781\n","[7000]\ttrain-logloss:0.186444\teval-logloss:0.214518\ttrain-amex:0.84464\teval-amex:0.795058\n","[7500]\ttrain-logloss:0.184485\teval-logloss:0.214378\ttrain-amex:0.848019\teval-amex:0.795275\n","[8000]\ttrain-logloss:0.182572\teval-logloss:0.214306\ttrain-amex:0.851386\teval-amex:0.79532\n","[8500]\ttrain-logloss:0.180697\teval-logloss:0.214217\ttrain-amex:0.854604\teval-amex:0.795521\n","[9000]\ttrain-logloss:0.178877\teval-logloss:0.214171\ttrain-amex:0.857629\teval-amex:0.795156\n","[9500]\ttrain-logloss:0.177068\teval-logloss:0.214109\ttrain-amex:0.860974\teval-amex:0.794968\n","[9999]\ttrain-logloss:0.17531\teval-logloss:0.214084\ttrain-amex:0.864007\teval-amex:0.795186\n","Model has been saved\n","best ntree_limit: 7879\n","best score: 0.796011\n","Our fold 0 CV score is 0.7957720592552006\n"," \n","--------------------------------------------------\n","Training fold 1 with 1365 features...\n","[0]\ttrain-logloss:0.686787\teval-logloss:0.686786\ttrain-amex:0.672374\teval-amex:0.671871\n","Multiple eval metrics have been passed: 'eval-amex' will be used for early stopping.\n","\n","Will train until eval-amex hasn't improved in 5000 rounds.\n","[500]\ttrain-logloss:0.236185\teval-logloss:0.239327\ttrain-amex:0.770495\teval-amex:0.765589\n","[1000]\ttrain-logloss:0.221056\teval-logloss:0.227366\ttrain-amex:0.788354\teval-amex:0.777793\n","[1500]\ttrain-logloss:0.214614\teval-logloss:0.223592\ttrain-amex:0.797578\teval-amex:0.781922\n","[2000]\ttrain-logloss:0.210302\teval-logloss:0.221682\ttrain-amex:0.804175\teval-amex:0.784283\n","[2500]\ttrain-logloss:0.206875\teval-logloss:0.220558\ttrain-amex:0.809553\teval-amex:0.786626\n","[3000]\ttrain-logloss:0.203905\teval-logloss:0.219763\ttrain-amex:0.814352\teval-amex:0.787108\n","[3500]\ttrain-logloss:0.201188\teval-logloss:0.219192\ttrain-amex:0.818986\teval-amex:0.788286\n","[4000]\ttrain-logloss:0.198679\teval-logloss:0.218783\ttrain-amex:0.822918\teval-amex:0.789572\n","[4500]\ttrain-logloss:0.196297\teval-logloss:0.218428\ttrain-amex:0.82698\teval-amex:0.789957\n","[5000]\ttrain-logloss:0.194085\teval-logloss:0.218155\ttrain-amex:0.830437\teval-amex:0.791087\n","[5500]\ttrain-logloss:0.191891\teval-logloss:0.217946\ttrain-amex:0.834368\teval-amex:0.791547\n","[6000]\ttrain-logloss:0.189773\teval-logloss:0.217749\ttrain-amex:0.837933\teval-amex:0.791848\n","[6500]\ttrain-logloss:0.187752\teval-logloss:0.217603\ttrain-amex:0.841639\teval-amex:0.792111\n","[7000]\ttrain-logloss:0.18579\teval-logloss:0.217517\ttrain-amex:0.84501\teval-amex:0.792569\n","[7500]\ttrain-logloss:0.183836\teval-logloss:0.217401\ttrain-amex:0.848344\teval-amex:0.792487\n","[8000]\ttrain-logloss:0.181935\teval-logloss:0.217331\ttrain-amex:0.851869\teval-amex:0.792239\n","[8500]\ttrain-logloss:0.180091\teval-logloss:0.217287\ttrain-amex:0.855465\teval-amex:0.792638\n","[9000]\ttrain-logloss:0.178324\teval-logloss:0.217263\ttrain-amex:0.858421\teval-amex:0.792668\n","[9500]\ttrain-logloss:0.176561\teval-logloss:0.217234\ttrain-amex:0.861679\teval-amex:0.792072\n","[9999]\ttrain-logloss:0.174786\teval-logloss:0.217193\ttrain-amex:0.864789\teval-amex:0.792235\n","Model has been saved\n","best ntree_limit: 7258\n","best score: 0.792894\n","Our fold 1 CV score is 0.7926546015812169\n"," \n","--------------------------------------------------\n","Training fold 2 with 1365 features...\n","[0]\ttrain-logloss:0.686776\teval-logloss:0.686781\ttrain-amex:0.675322\teval-amex:0.671489\n","Multiple eval metrics have been passed: 'eval-amex' will be used for early stopping.\n","\n","Will train until eval-amex hasn't improved in 5000 rounds.\n","[500]\ttrain-logloss:0.236598\teval-logloss:0.238047\ttrain-amex:0.769326\teval-amex:0.768894\n","[1000]\ttrain-logloss:0.221621\teval-logloss:0.225295\ttrain-amex:0.787345\teval-amex:0.783048\n","[1500]\ttrain-logloss:0.215271\teval-logloss:0.22116\ttrain-amex:0.795909\teval-amex:0.787396\n","[2000]\ttrain-logloss:0.210976\teval-logloss:0.219053\ttrain-amex:0.802681\teval-amex:0.79056\n","[2500]\ttrain-logloss:0.207542\teval-logloss:0.21781\ttrain-amex:0.80837\teval-amex:0.79267\n","[3000]\ttrain-logloss:0.204552\teval-logloss:0.21699\ttrain-amex:0.813088\teval-amex:0.793729\n","[3500]\ttrain-logloss:0.201845\teval-logloss:0.216414\ttrain-amex:0.817442\teval-amex:0.794264\n","[4000]\ttrain-logloss:0.199317\teval-logloss:0.215973\ttrain-amex:0.821937\teval-amex:0.794899\n","[4500]\ttrain-logloss:0.196956\teval-logloss:0.215628\ttrain-amex:0.825975\teval-amex:0.795292\n","[5000]\ttrain-logloss:0.194676\teval-logloss:0.215356\ttrain-amex:0.829677\teval-amex:0.795363\n","[5500]\ttrain-logloss:0.192492\teval-logloss:0.215148\ttrain-amex:0.833442\teval-amex:0.795641\n","[6000]\ttrain-logloss:0.190404\teval-logloss:0.215003\ttrain-amex:0.836677\teval-amex:0.795839\n","[6500]\ttrain-logloss:0.188359\teval-logloss:0.214858\ttrain-amex:0.840691\teval-amex:0.796311\n","[7000]\ttrain-logloss:0.186376\teval-logloss:0.214737\ttrain-amex:0.844233\teval-amex:0.796436\n","[7500]\ttrain-logloss:0.184448\teval-logloss:0.214643\ttrain-amex:0.847495\teval-amex:0.796955\n","[8000]\ttrain-logloss:0.18254\teval-logloss:0.214576\ttrain-amex:0.850982\teval-amex:0.797359\n","[8500]\ttrain-logloss:0.180671\teval-logloss:0.214519\ttrain-amex:0.854178\teval-amex:0.797487\n","[9000]\ttrain-logloss:0.178849\teval-logloss:0.214479\ttrain-amex:0.857455\teval-amex:0.797563\n","[9500]\ttrain-logloss:0.177057\teval-logloss:0.214431\ttrain-amex:0.86061\teval-amex:0.797477\n","[9999]\ttrain-logloss:0.175309\teval-logloss:0.214399\ttrain-amex:0.863958\teval-amex:0.79806\n","Model has been saved\n","best ntree_limit: 9839\n","best score: 0.798265\n","Our fold 2 CV score is 0.7980264796238685\n"," \n","--------------------------------------------------\n","Training fold 3 with 1365 features...\n","[0]\ttrain-logloss:0.686782\teval-logloss:0.6868\ttrain-amex:0.671832\teval-amex:0.670895\n","Multiple eval metrics have been passed: 'eval-amex' will be used for early stopping.\n","\n","Will train until eval-amex hasn't improved in 5000 rounds.\n","[500]\ttrain-logloss:0.235972\teval-logloss:0.240374\ttrain-amex:0.770628\teval-amex:0.764666\n","[1000]\ttrain-logloss:0.220884\teval-logloss:0.228025\ttrain-amex:0.788004\teval-amex:0.778319\n","[1500]\ttrain-logloss:0.214493\teval-logloss:0.224021\ttrain-amex:0.797108\teval-amex:0.782308\n","[2000]\ttrain-logloss:0.210207\teval-logloss:0.222016\ttrain-amex:0.80393\teval-amex:0.785466\n","[2500]\ttrain-logloss:0.206775\teval-logloss:0.220766\ttrain-amex:0.809495\teval-amex:0.787369\n","[3000]\ttrain-logloss:0.203833\teval-logloss:0.219957\ttrain-amex:0.814095\teval-amex:0.788374\n","[3500]\ttrain-logloss:0.201158\teval-logloss:0.219364\ttrain-amex:0.818613\teval-amex:0.788734\n","[4000]\ttrain-logloss:0.198666\teval-logloss:0.218923\ttrain-amex:0.822874\teval-amex:0.789082\n","[4500]\ttrain-logloss:0.196295\teval-logloss:0.218572\ttrain-amex:0.826703\teval-amex:0.789943\n","[5000]\ttrain-logloss:0.194059\teval-logloss:0.218303\ttrain-amex:0.830423\teval-amex:0.790565\n","[5500]\ttrain-logloss:0.191887\teval-logloss:0.218054\ttrain-amex:0.834051\teval-amex:0.790634\n","[6000]\ttrain-logloss:0.189807\teval-logloss:0.217851\ttrain-amex:0.838041\teval-amex:0.790307\n","[6500]\ttrain-logloss:0.187742\teval-logloss:0.2177\ttrain-amex:0.841129\teval-amex:0.790951\n","[7000]\ttrain-logloss:0.185762\teval-logloss:0.217599\ttrain-amex:0.844721\teval-amex:0.790882\n","[7500]\ttrain-logloss:0.183836\teval-logloss:0.21751\ttrain-amex:0.847897\teval-amex:0.791104\n","[8000]\ttrain-logloss:0.181917\teval-logloss:0.217396\ttrain-amex:0.851377\teval-amex:0.791506\n","[8500]\ttrain-logloss:0.180066\teval-logloss:0.217326\ttrain-amex:0.854882\teval-amex:0.791679\n","[9000]\ttrain-logloss:0.178227\teval-logloss:0.217271\ttrain-amex:0.85826\teval-amex:0.792102\n","[9500]\ttrain-logloss:0.176438\teval-logloss:0.217236\ttrain-amex:0.861396\teval-amex:0.791927\n","[9999]\ttrain-logloss:0.174659\teval-logloss:0.21719\ttrain-amex:0.86463\teval-amex:0.79203\n","Model has been saved\n","best ntree_limit: 9178\n","best score: 0.792633\n","Our fold 3 CV score is 0.7924218158882248\n"," \n","--------------------------------------------------\n","Training fold 4 with 1365 features...\n","[0]\ttrain-logloss:0.686777\teval-logloss:0.686792\ttrain-amex:0.68036\teval-amex:0.681929\n","Multiple eval metrics have been passed: 'eval-amex' will be used for early stopping.\n","\n","Will train until eval-amex hasn't improved in 5000 rounds.\n","[500]\ttrain-logloss:0.236429\teval-logloss:0.238816\ttrain-amex:0.769548\teval-amex:0.769185\n","[1000]\ttrain-logloss:0.221448\teval-logloss:0.225882\ttrain-amex:0.786597\teval-amex:0.784173\n","[1500]\ttrain-logloss:0.215071\teval-logloss:0.221918\ttrain-amex:0.795354\teval-amex:0.78948\n","[2000]\ttrain-logloss:0.210724\teval-logloss:0.219947\ttrain-amex:0.802219\teval-amex:0.79184\n","[2500]\ttrain-logloss:0.207324\teval-logloss:0.218816\ttrain-amex:0.807734\teval-amex:0.794083\n","[3000]\ttrain-logloss:0.204302\teval-logloss:0.21801\ttrain-amex:0.812664\teval-amex:0.794348\n","[3500]\ttrain-logloss:0.201582\teval-logloss:0.21746\ttrain-amex:0.817357\teval-amex:0.795008\n","[4000]\ttrain-logloss:0.199037\teval-logloss:0.217046\ttrain-amex:0.821627\teval-amex:0.795072\n","[4500]\ttrain-logloss:0.196641\teval-logloss:0.216755\ttrain-amex:0.825821\teval-amex:0.795493\n","[5000]\ttrain-logloss:0.194356\teval-logloss:0.2165\ttrain-amex:0.829798\teval-amex:0.795583\n","[5500]\ttrain-logloss:0.192187\teval-logloss:0.216259\ttrain-amex:0.83336\teval-amex:0.796426\n","[6000]\ttrain-logloss:0.190039\teval-logloss:0.216115\ttrain-amex:0.837018\teval-amex:0.796394\n","[6500]\ttrain-logloss:0.187954\teval-logloss:0.215976\ttrain-amex:0.840544\teval-amex:0.796594\n","[7000]\ttrain-logloss:0.185947\teval-logloss:0.215883\ttrain-amex:0.844015\teval-amex:0.79707\n","[7500]\ttrain-logloss:0.183964\teval-logloss:0.215782\ttrain-amex:0.847584\teval-amex:0.797173\n","[8000]\ttrain-logloss:0.182079\teval-logloss:0.215731\ttrain-amex:0.851075\teval-amex:0.79671\n","[8500]\ttrain-logloss:0.180226\teval-logloss:0.215684\ttrain-amex:0.854457\teval-amex:0.796982\n","[9000]\ttrain-logloss:0.17843\teval-logloss:0.215619\ttrain-amex:0.857325\teval-amex:0.797471\n","[9500]\ttrain-logloss:0.176643\teval-logloss:0.215555\ttrain-amex:0.860479\teval-amex:0.797685\n","[9999]\ttrain-logloss:0.174901\teval-logloss:0.215501\ttrain-amex:0.863895\teval-amex:0.797328\n","Model has been saved\n","best ntree_limit: 9700\n","best score: 0.797842\n","Our fold 4 CV score is 0.7976305109328017\n","Our out of folds CV score is 0.7947842269947178\n"]}],"source":["# ====================================================\n","# Library\n","# ====================================================\n","import os\n","import gc\n","import warnings\n","warnings.filterwarnings('ignore')\n","import random\n","import scipy as sp\n","import numpy as np\n","import pandas as pd\n","import joblib\n","import itertools\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","from tqdm.auto import tqdm\n","from sklearn.model_selection import StratifiedKFold, train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import lightgbm as lgb\n","from itertools import combinations\n","\n","\n","\n","# ====================================================\n","# Seed everything\n","# ====================================================\n","def seed_everything(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","\n","# ====================================================\n","# Read data\n","# ====================================================\n","def read_data():\n","    train = pd.read_parquet(CFG.data_dir + 'train_fe.parquet')\n","    test = pd.read_parquet(CFG.data_dir + 'test_fe.parquet')\n","    return train, test\n","\n","# ====================================================\n","# Amex metric\n","# ====================================================\n","def amex_metric(y_true, y_pred):\n","    labels = np.transpose(np.array([y_true, y_pred]))\n","    labels = labels[labels[:, 1].argsort()[::-1]]\n","    weights = np.where(labels[:,0]==0, 20, 1)\n","    cut_vals = labels[np.cumsum(weights) \u003c= int(0.04 * np.sum(weights))]\n","    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n","    gini = [0,0]\n","    for i in [1,0]:\n","        labels = np.transpose(np.array([y_true, y_pred]))\n","        labels = labels[labels[:, i].argsort()[::-1]]\n","        weight = np.where(labels[:,0]==0, 20, 1)\n","        weight_random = np.cumsum(weight / np.sum(weight))\n","        total_pos = np.sum(labels[:, 0] *  weight)\n","        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n","        lorentz = cum_pos_found / total_pos\n","        gini[i] = np.sum((lorentz - weight_random) * weight)\n","    return 0.5 * (gini[1]/gini[0] + top_four)\n","\n","# ====================================================\n","# LGBM amex metric\n","# ====================================================\n","def lgb_amex_metric(y_pred, y_true):\n","    y_true = y_true.get_label()\n","    return 'amex_metric', amex_metric(y_true, y_pred), True\n","\n","def xgb_amex(y_pred, y_true):\n","    return 'amex', amex_metric_np(y_pred,y_true.get_label())\n","def amex_metric_np(preds: np.ndarray, target: np.ndarray) -\u003e float:\n","    indices = np.argsort(preds)[::-1]\n","    preds, target = preds[indices], target[indices]\n","\n","    weight = 20.0 - target * 19.0\n","    cum_norm_weight = (weight / weight.sum()).cumsum()\n","    four_pct_mask = cum_norm_weight \u003c= 0.04\n","    d = np.sum(target[four_pct_mask]) / np.sum(target)\n","\n","    weighted_target = target * weight\n","    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n","    gini = ((lorentz - cum_norm_weight) * weight).sum()\n","\n","    n_pos = np.sum(target)\n","    n_neg = target.shape[0] - n_pos\n","    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n","\n","    g = gini / gini_max\n","    return 0.5 * (g + d)\n","\n","# ====================================================\n","# Train \u0026 Evaluate\n","# ====================================================\n","def train_and_evaluate(train, test):\n","    # Label encode categorical features\n","    cat_features = [\n","        \"B_30\",\n","        \"B_38\",\n","        \"D_114\",\n","        \"D_116\",\n","        \"D_117\",\n","        \"D_120\",\n","        \"D_126\",\n","        \"D_63\",\n","        \"D_64\",\n","        \"D_66\",\n","        \"D_68\"\n","    ]\n","    cat_features = [f\"{cf}_last\" for cf in cat_features]\n","    for cat_col in cat_features:\n","        encoder = LabelEncoder()\n","        train[cat_col] = encoder.fit_transform(train[cat_col])\n","        test[cat_col] = encoder.transform(test[cat_col])\n","    # Round last float features to 2 decimal place\n","    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n","    num_cols = [col for col in num_cols if 'last' in col]\n","    for col in num_cols:\n","        train[col + '_round2'] = train[col].round(2)\n","        test[col + '_round2'] = test[col].round(2)\n","    # Get the difference between last and mean\n","    num_cols = [col for col in train.columns if 'last' in col]\n","    num_cols = [col[:-5] for col in num_cols if 'round' not in col]\n","    for col in num_cols:\n","        try:\n","            train[f'{col}_last_mean_diff'] = train[f'{col}_last'] - train[f'{col}_mean']\n","            test[f'{col}_last_mean_diff'] = test[f'{col}_last'] - test[f'{col}_mean']\n","        except:\n","            pass\n","    # Transform float64 and float32 to float16\n","    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n","    for col in tqdm(num_cols):\n","        train[col] = train[col].astype(np.float16)\n","        test[col] = test[col].astype(np.float16)\n","    # Get feature list\n","    features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n","    # params = {\n","    #     'objective': 'binary',\n","    #     'metric': CFG.metric,\n","    #     'boosting': CFG.boosting_type,\n","    #     'seed': CFG.seed,\n","    #     'num_leaves': 120,#100,\n","    #     'learning_rate': 0.01,\n","    #     'feature_fraction': 0.20,\n","    #     'bagging_freq': 10,\n","    #     'bagging_fraction': 0.50,\n","    #     'n_jobs': -1,\n","    #     'lambda_l2': 2,\n","    #     'min_data_in_leaf': 40,\n","    #     }\n","    params = {\n","      'max_depth':4, \n","      'learning_rate':0.01, \n","      'subsample':0.8,\n","      'colsample_bytree':0.6, \n","      'eval_metric':'logloss',\n","      'objective':'binary:logistic',\n","      'tree_method':'gpu_hist',\n","      'predictor':'gpu_predictor',\n","      'random_state':CFG.seed\n","      }\n","\n","    # Create a numpy array to store test predictions\n","    test_predictions = np.zeros(len(test))\n","    # Create a numpy array to store out of folds predictions\n","    oof_predictions = np.zeros(len(train))\n","    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n","    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n","        print(' ')\n","        print('-'*50)\n","        print(f'Training fold {fold} with {len(features)} features...')\n","        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n","        y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n","        dtrain = xgb.DMatrix(data = x_train, label = y_train)\n","        dvalid = xgb.DMatrix(data = x_val, label = y_val)        \n","        # lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n","        # lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n","        # model = lgb.train(\n","        #     params = params,\n","        #     train_set = lgb_train,\n","        #     num_boost_round = 10500,\n","        #     valid_sets = [lgb_train, lgb_valid],\n","        #     early_stopping_rounds = 1500,\n","        #     verbose_eval = 500,\n","        #     feval = lgb_amex_metric\n","        #     )\n","        \n","        # clf = CatBoostClassifier(iterations=2000, random_state=CFG.seed, early_stopping_rounds=200, task_type='GPU')\n","        # clf = CatBoostClassifier(iterations=5000, random_state=CFG.seed, nan_mode='Min', eval_metric=CB_Amex_Metric(), depth=8,use_best_model=True)\n","        # clf = CatBoostClassifier(iterations=5000, random_state=CFG.seed,  eval_metric=CB_Amex_Metric(), use_best_model=True)\n","\n","        # clf.fit(x_train, y_train, eval_set=[(x_val, y_val)], cat_features=cat_features,verbose=200)\n","        watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n","        clf = xgb.train(params, dtrain = dtrain, num_boost_round = 10000, evals = watchlist, early_stopping_rounds = 5000, feval = xgb_amex, maximize = True, verbose_eval = 500)\n","       \n","        # Save best model\n","#         joblib.dump(model, f'/content/drive/MyDrive/Amex/Models/lgbm_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n","#         joblib.dump(model, f'lgbm_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n","        clf.save_model(f'{CFG.input_dir}xgboost_amex_model_{fold}.cbm')\n","        print('Model has been saved')\n","        print('best ntree_limit:', clf.best_ntree_limit)\n","        print('best score:', clf.best_score) \n","        # Save best model\n","#         joblib.dump(model, f'/content/drive/MyDrive/Amex/Models/lgbm_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n","        # joblib.dump(model, f'{CFG.input_dir}lgbm_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n","        \n","        # Predict validation\n","        # val_pred = model.predict(x_val)\n","        # val_pred = clf.predict_proba(x_val)[:, 1]\n","        val_pred = clf.predict(xgb.DMatrix(x_val), ntree_limit=clf.best_ntree_limit)\n","\n","        # Add to out of folds array\n","        oof_predictions[val_ind] = val_pred\n","        # Predict the test set\n","        # test_pred = model.predict(test[features])\n","        # test_pred = clf.predict_proba(test[features])[:, 1]\n","        test_pred = clf.predict(xgb.DMatrix(test[features]), ntree_limit = clf.best_ntree_limit)\n","\n","        test_predictions += test_pred / CFG.n_folds\n","        # Compute fold metric\n","        score = amex_metric(y_val, val_pred)\n","        print(f'Our fold {fold} CV score is {score}')\n","        del x_train, x_val, y_train, y_val, dtrain, dvalid\n","        gc.collect()\n","    # Compute out of folds metric\n","    score = amex_metric(train[CFG.target], oof_predictions)\n","    print(f'Our out of folds CV score is {score}')\n","    # Create a dataframe to store out of folds predictions\n","    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n","#     oof_df.to_csv(f'/content/drive/MyDrive/Amex/OOF/oof_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n","    oof_df.to_csv(f'{CFG.input_dir}oof_xgb_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n","    \n","    # Create a dataframe to store test prediction\n","    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n","#     test_df.to_csv(f'/content/drive/MyDrive/Amex/Predictions/test_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n","    test_df.to_csv(f'{CFG.input_dir}test_xgboost_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n","    \n","seed_everything(CFG.seed)\n","train, test = read_data()\n","train_and_evaluate(train, test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"H4h4IGSdZByf"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"hQKlH5vuVNZF"},"source":["# Read Submission File\n","This is the submission file corresponding to the output of the previous pipeline (using the average blend of 3 seeds)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"WXqsUUenVNZF"},"outputs":[],"source":["# sub = pd.read_csv('../input/amex-sub/test_lgbm_baseline_5fold_seed_blend.csv')\n","# sub.to_csv('test_lgbm_baseline_5fold_seed_blend.csv', index = False)"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"catboost4 of amex-lgbm-dart-cv-0-7977.ipynb","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0b64dca6a6dd4e4581f6782d623aa3ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9a973966ecc42dda912fa03e0d8c5e9","placeholder":"​","style":"IPY_MODEL_664a536096334aed85e83e9a71110efd","value":"  1%"}},"1a79ef50e6244d65bb082f43c4c39a8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ff7ad9c96fd447981fc7ce9187e8213","max":1080,"min":0,"orientation":"horizontal","style":"IPY_MODEL_527894773a5f452382cb71387363ce5e","value":10}},"25a5a55dffee490ea0b7bca7c9dc1ffd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"286eb39da7f54bc8be8458a9a2d99ea1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8a3831778d74cda87caca45f42de9e0","placeholder":"​","style":"IPY_MODEL_2e7cb37f08ab4ca4977c86dadc3010f6","value":" 10/1080 [00:10\u0026lt;17:03,  1.05it/s]"}},"2e7cb37f08ab4ca4977c86dadc3010f6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"527894773a5f452382cb71387363ce5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5679eb42c42740f48c05ad35672f008e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0b64dca6a6dd4e4581f6782d623aa3ef","IPY_MODEL_1a79ef50e6244d65bb082f43c4c39a8e","IPY_MODEL_286eb39da7f54bc8be8458a9a2d99ea1"],"layout":"IPY_MODEL_25a5a55dffee490ea0b7bca7c9dc1ffd"}},"664a536096334aed85e83e9a71110efd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9ff7ad9c96fd447981fc7ce9187e8213":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9a973966ecc42dda912fa03e0d8c5e9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8a3831778d74cda87caca45f42de9e0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}